{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, RandomForestRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import log\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "train = pd.read_csv('../datasets/train.csv')\n",
    "test = pd.read_csv('../datasets/test.csv', index_col=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "outputs": [],
   "source": [
    "from cmath import isnan\n",
    "\n",
    "\n",
    "def process_df(df: pd.DataFrame, is_train: bool) -> pd.DataFrame:\n",
    "    df = df.drop('Color', axis=1)\n",
    "    df['Rating'] = df['Rating'].str.replace(',', '.')\n",
    "    df['Rating'] = df['Rating'].astype(float)\n",
    "\n",
    "    # if is_train:\n",
    "    #     df['Brand weight'] = \"\"\n",
    "    #     for category in df['Brand'].unique():\n",
    "    #         total_rating = df.loc[df['Brand'] == category].count() / df.shape[0]\n",
    "    #         df.loc[df['Brand'] == category, 'Brand weight'] = total_rating\n",
    "\n",
    "    df = df.drop([\n",
    "        'Name',\n",
    "        'Category',\n",
    "        # 'Min price',\n",
    "        # 'Max price',\n",
    "        # 'Average price',\n",
    "        # 'Basic Sale',\n",
    "        'full_category',\n",
    "        'Seller',\n",
    "        'Base price'\n",
    "    ], axis=1)\n",
    "    df['Days in stock/sales'] = df['Days with sales'].div(df['Days in stock'], axis=0).apply(\n",
    "        lambda x: 1.0 if x >= 1.0 else 0 if isnan(x) else x)\n",
    "    df['Comments-Rating'] = df['Comments'] * df['Rating']\n",
    "    df['Rating-Days-Comments'] = np.exp(df['Days with sales']) * df['Rating'] * df['Comments']\n",
    "    df['Price difference'] = np.exp(df['Average price'].fillna(0).div(df['Final price'], axis=0)) ** 3\n",
    "    df['Min max price diff'] = np.log10((df['Max price'] - df['Final price']).apply(lambda x: 1 if x == 0 else x)) ** 3\n",
    "    df['Days_Relation-Rating-Comments'] = df['Days in stock/sales'] * (df['Rating'] ** 2 * df['Comments']).apply(lambda x: 1 if x == 0 else x)\n",
    "    df = df.drop(['Average price', 'Max price', 'Min price'], axis=1)\n",
    "\n",
    "    return df.dropna() if is_train else df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "outputs": [
    {
     "data": {
      "text/plain": "            Brand  Comments  Final price  Days in stock  Days with sales  \\\n0          Majava       2.0        277.0           16.0               14   \n1            Beon       5.0       2211.0            7.0                8   \n2          NEOLUX      19.0        490.0           31.0               22   \n3          CENTEK       2.0        807.0           31.0                0   \n4           HUAYU       3.0        426.0           31.0                5   \n...           ...       ...          ...            ...              ...   \n10736     INDESIT       0.0      17803.0           31.0                3   \n10737     Gelberk       8.0        897.0           12.0                9   \n10738     Rowenta      15.0       1565.0           31.0               29   \n10739      EREMON       0.0       2752.0           11.0                2   \n10740  Electrolux       0.0      16566.0           24.0                9   \n\n       Rating  Basic Sale  Basic Sale Price  Days in stock/sales  \\\n0         0.0        50.0             360.0             0.875000   \n1         3.0        40.0            2664.0             1.000000   \n2         5.0        30.0             490.0             0.709677   \n3         4.0        15.0             807.0             0.000000   \n4         5.0         3.0             426.0             0.161290   \n...       ...         ...               ...                  ...   \n10736     0.0        23.0           19781.0             0.096774   \n10737     4.0        54.0            1150.0             0.750000   \n10738     3.0        46.0            1565.0             0.935484   \n10739     0.0        20.0            3440.0             0.181818   \n10740     0.0         0.0           19490.0             0.375000   \n\n       Comments-Rating  Rating-Days-Comments  Price difference  \\\n0                  0.0          0.000000e+00         17.885865   \n1                 15.0          4.471437e+04         20.085537   \n2                 95.0          3.405667e+11         18.535758   \n3                  8.0          8.000000e+00          1.000000   \n4                 15.0          2.226197e+03         20.085537   \n...                ...                   ...               ...   \n10736              0.0          0.000000e+00         20.085537   \n10737             32.0          2.592987e+05         20.085537   \n10738             45.0          1.769100e+14         27.715964   \n10739              0.0          0.000000e+00         20.085537   \n10740              0.0          0.000000e+00         20.085537   \n\n       Min max price diff  Rating-Price  \n0                0.000000      0.875000  \n1                0.000000     45.000000  \n2                0.000000    337.096774  \n3                0.000000      0.000000  \n4                1.505565     12.096774  \n...                   ...           ...  \n10736           42.065928      0.096774  \n10737            0.000000     96.000000  \n10738           30.522144    126.290323  \n10739            0.000000      0.181818  \n10740            0.000000      0.375000  \n\n[10741 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Brand</th>\n      <th>Comments</th>\n      <th>Final price</th>\n      <th>Days in stock</th>\n      <th>Days with sales</th>\n      <th>Rating</th>\n      <th>Basic Sale</th>\n      <th>Basic Sale Price</th>\n      <th>Days in stock/sales</th>\n      <th>Comments-Rating</th>\n      <th>Rating-Days-Comments</th>\n      <th>Price difference</th>\n      <th>Min max price diff</th>\n      <th>Rating-Price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Majava</td>\n      <td>2.0</td>\n      <td>277.0</td>\n      <td>16.0</td>\n      <td>14</td>\n      <td>0.0</td>\n      <td>50.0</td>\n      <td>360.0</td>\n      <td>0.875000</td>\n      <td>0.0</td>\n      <td>0.000000e+00</td>\n      <td>17.885865</td>\n      <td>0.000000</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Beon</td>\n      <td>5.0</td>\n      <td>2211.0</td>\n      <td>7.0</td>\n      <td>8</td>\n      <td>3.0</td>\n      <td>40.0</td>\n      <td>2664.0</td>\n      <td>1.000000</td>\n      <td>15.0</td>\n      <td>4.471437e+04</td>\n      <td>20.085537</td>\n      <td>0.000000</td>\n      <td>45.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NEOLUX</td>\n      <td>19.0</td>\n      <td>490.0</td>\n      <td>31.0</td>\n      <td>22</td>\n      <td>5.0</td>\n      <td>30.0</td>\n      <td>490.0</td>\n      <td>0.709677</td>\n      <td>95.0</td>\n      <td>3.405667e+11</td>\n      <td>18.535758</td>\n      <td>0.000000</td>\n      <td>337.096774</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CENTEK</td>\n      <td>2.0</td>\n      <td>807.0</td>\n      <td>31.0</td>\n      <td>0</td>\n      <td>4.0</td>\n      <td>15.0</td>\n      <td>807.0</td>\n      <td>0.000000</td>\n      <td>8.0</td>\n      <td>8.000000e+00</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HUAYU</td>\n      <td>3.0</td>\n      <td>426.0</td>\n      <td>31.0</td>\n      <td>5</td>\n      <td>5.0</td>\n      <td>3.0</td>\n      <td>426.0</td>\n      <td>0.161290</td>\n      <td>15.0</td>\n      <td>2.226197e+03</td>\n      <td>20.085537</td>\n      <td>1.505565</td>\n      <td>12.096774</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>10736</th>\n      <td>INDESIT</td>\n      <td>0.0</td>\n      <td>17803.0</td>\n      <td>31.0</td>\n      <td>3</td>\n      <td>0.0</td>\n      <td>23.0</td>\n      <td>19781.0</td>\n      <td>0.096774</td>\n      <td>0.0</td>\n      <td>0.000000e+00</td>\n      <td>20.085537</td>\n      <td>42.065928</td>\n      <td>0.096774</td>\n    </tr>\n    <tr>\n      <th>10737</th>\n      <td>Gelberk</td>\n      <td>8.0</td>\n      <td>897.0</td>\n      <td>12.0</td>\n      <td>9</td>\n      <td>4.0</td>\n      <td>54.0</td>\n      <td>1150.0</td>\n      <td>0.750000</td>\n      <td>32.0</td>\n      <td>2.592987e+05</td>\n      <td>20.085537</td>\n      <td>0.000000</td>\n      <td>96.000000</td>\n    </tr>\n    <tr>\n      <th>10738</th>\n      <td>Rowenta</td>\n      <td>15.0</td>\n      <td>1565.0</td>\n      <td>31.0</td>\n      <td>29</td>\n      <td>3.0</td>\n      <td>46.0</td>\n      <td>1565.0</td>\n      <td>0.935484</td>\n      <td>45.0</td>\n      <td>1.769100e+14</td>\n      <td>27.715964</td>\n      <td>30.522144</td>\n      <td>126.290323</td>\n    </tr>\n    <tr>\n      <th>10739</th>\n      <td>EREMON</td>\n      <td>0.0</td>\n      <td>2752.0</td>\n      <td>11.0</td>\n      <td>2</td>\n      <td>0.0</td>\n      <td>20.0</td>\n      <td>3440.0</td>\n      <td>0.181818</td>\n      <td>0.0</td>\n      <td>0.000000e+00</td>\n      <td>20.085537</td>\n      <td>0.000000</td>\n      <td>0.181818</td>\n    </tr>\n    <tr>\n      <th>10740</th>\n      <td>Electrolux</td>\n      <td>0.0</td>\n      <td>16566.0</td>\n      <td>24.0</td>\n      <td>9</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>19490.0</td>\n      <td>0.375000</td>\n      <td>0.0</td>\n      <td>0.000000e+00</td>\n      <td>20.085537</td>\n      <td>0.000000</td>\n      <td>0.375000</td>\n    </tr>\n  </tbody>\n</table>\n<p>10741 rows × 14 columns</p>\n</div>"
     },
     "execution_count": 1012,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin_train = process_df(train, True)\n",
    "origin_test = process_df(test, False)\n",
    "origin_test.drop('Id', axis=1, inplace=True)\n",
    "\n",
    "origin_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "outputs": [
    {
     "data": {
      "text/plain": "(10741, 14)"
     },
     "execution_count": 1013,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin_test.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "outputs": [
    {
     "data": {
      "text/plain": "(25061, 15)"
     },
     "execution_count": 1014,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin_train.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1015,
   "outputs": [],
   "source": [
    "def get_histogram(df: pd.DataFrame, name: str):\n",
    "    vals = sorted(df[name].unique())\n",
    "\n",
    "    bins = 1 + log(len(vals), 2)\n",
    "    min = vals[0]\n",
    "    max = vals[len(vals) - 1]\n",
    "    length = (max - min) / bins\n",
    "\n",
    "    intervals = [min + length * i for i in range(int(bins) + 1)]\n",
    "    plt.hist(vals, histtype='stepfilled', bins=intervals)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1016,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_histogram(origin_train, 'Final price')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1017,
   "outputs": [],
   "source": [
    "features = origin_train.drop(['Sales'], axis=1)\n",
    "target = origin_train['Sales']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.33, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1018,
   "outputs": [],
   "source": [
    "# def clear_column_from_extremes(data_frame: pd.DataFrame, col_name: str):\n",
    "#     column = data_frame[col_name]\n",
    "#     q = np.nanquantile(column, q=[0.25, 0.75])\n",
    "#     low = q[0] - 1.5 * (q[1] - q[0])\n",
    "#     high = q[1] + 1.5 * (q[1] - q[0])\n",
    "#\n",
    "#     for index, row in data_frame.iterrows():\n",
    "#         aux = data_frame[col_name].loc[index]\n",
    "#         if not (low <= aux < high) and np.isnan(data_frame[\"G_total\"].loc[index]):\n",
    "#            data_frame.drop(index, inplace=True)\n",
    "#     return data_frame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1019,
   "outputs": [],
   "source": [
    "def delete_nan_cols(df: pd.DataFrame):\n",
    "    new_df = df\n",
    "    nan_count_series = new_df.isna().sum()\n",
    "    column_size = new_df.shape[0]\n",
    "\n",
    "    for i, v in nan_count_series.items():\n",
    "        if (v / column_size) * 100 > 45:\n",
    "            print(f'Deleted {i}')\n",
    "            new_df = new_df.drop(i, axis=1)\n",
    "\n",
    "    return new_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1020,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "print(len(list(delete_nan_cols(X_train).columns.values)))\n",
    "print(len(list(X_train.columns.values)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1021,
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "outputs": [],
   "source": [
    "def categorize_columns(data: pd.DataFrame):\n",
    "    categorical_columns = data.columns[data.dtypes == 'object']\n",
    "    print(categorical_columns)\n",
    "\n",
    "    for column in categorical_columns:\n",
    "        data[column] = label_encoder.fit_transform(data[column])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Brand'], dtype='object')\n",
      "Index(['Brand'], dtype='object')\n",
      "Index(['Brand'], dtype='object')\n",
      "Index(['Brand'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "categorize_columns(X_train)\n",
    "categorize_columns(X_test)\n",
    "categorize_columns(origin_test)\n",
    "categorize_columns(origin_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1024,
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot: >"
     },
     "execution_count": 1024,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cor = X_train.corr()\n",
    "plt.clf()\n",
    "sns.heatmap(cor, xticklabels=range(X_train.shape[1]), yticklabels=1, linewidths=.5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1025,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "building tree 1 of 100\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "building tree 2 of 100\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.2s remaining:    0.0s\n",
      "building tree 3 of 100\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.3s remaining:    0.0s\n",
      "building tree 4 of 100\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.4s remaining:    0.0s\n",
      "building tree 5 of 100\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.4s remaining:    0.0s\n",
      "building tree 6 of 100\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    0.5s remaining:    0.0s\n",
      "building tree 7 of 100\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.6s remaining:    0.0s\n",
      "building tree 8 of 100\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.7s remaining:    0.0s\n",
      "building tree 9 of 100\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    0.8s remaining:    0.0s\n",
      "building tree 10 of 100\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.9s remaining:    0.0s\n",
      "building tree 11 of 100\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:    1.0s remaining:    0.0s\n",
      "building tree 12 of 100\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    1.1s remaining:    0.0s\n",
      "building tree 13 of 100\n",
      "[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed:    1.2s remaining:    0.0s\n",
      "building tree 14 of 100\n",
      "[Parallel(n_jobs=1)]: Done  14 out of  14 | elapsed:    1.3s remaining:    0.0s\n",
      "building tree 15 of 100\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    1.3s remaining:    0.0s\n",
      "building tree 16 of 100\n",
      "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed:    1.4s remaining:    0.0s\n",
      "building tree 17 of 100\n",
      "[Parallel(n_jobs=1)]: Done  17 out of  17 | elapsed:    1.5s remaining:    0.0s\n",
      "building tree 18 of 100\n",
      "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed:    1.6s remaining:    0.0s\n",
      "building tree 19 of 100\n",
      "[Parallel(n_jobs=1)]: Done  19 out of  19 | elapsed:    1.7s remaining:    0.0s\n",
      "building tree 20 of 100\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    1.8s remaining:    0.0s\n",
      "building tree 21 of 100\n",
      "[Parallel(n_jobs=1)]: Done  21 out of  21 | elapsed:    1.9s remaining:    0.0s\n",
      "building tree 22 of 100\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    2.0s remaining:    0.0s\n",
      "building tree 23 of 100\n",
      "[Parallel(n_jobs=1)]: Done  23 out of  23 | elapsed:    2.0s remaining:    0.0s\n",
      "building tree 24 of 100\n",
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:    2.1s remaining:    0.0s\n",
      "building tree 25 of 100\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    2.2s remaining:    0.0s\n",
      "building tree 26 of 100\n",
      "[Parallel(n_jobs=1)]: Done  26 out of  26 | elapsed:    2.3s remaining:    0.0s\n",
      "building tree 27 of 100\n",
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:    2.4s remaining:    0.0s\n",
      "building tree 28 of 100\n",
      "[Parallel(n_jobs=1)]: Done  28 out of  28 | elapsed:    2.5s remaining:    0.0s\n",
      "building tree 29 of 100\n",
      "[Parallel(n_jobs=1)]: Done  29 out of  29 | elapsed:    2.6s remaining:    0.0s\n",
      "building tree 30 of 100\n",
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:    2.7s remaining:    0.0s\n",
      "building tree 31 of 100\n",
      "[Parallel(n_jobs=1)]: Done  31 out of  31 | elapsed:    2.8s remaining:    0.0s\n",
      "building tree 32 of 100\n",
      "[Parallel(n_jobs=1)]: Done  32 out of  32 | elapsed:    2.8s remaining:    0.0s\n",
      "building tree 33 of 100\n",
      "[Parallel(n_jobs=1)]: Done  33 out of  33 | elapsed:    2.9s remaining:    0.0s\n",
      "building tree 34 of 100\n",
      "[Parallel(n_jobs=1)]: Done  34 out of  34 | elapsed:    3.0s remaining:    0.0s\n",
      "building tree 35 of 100\n",
      "[Parallel(n_jobs=1)]: Done  35 out of  35 | elapsed:    3.1s remaining:    0.0s\n",
      "building tree 36 of 100\n",
      "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed:    3.2s remaining:    0.0s\n",
      "building tree 37 of 100\n",
      "[Parallel(n_jobs=1)]: Done  37 out of  37 | elapsed:    3.3s remaining:    0.0s\n",
      "building tree 38 of 100\n",
      "[Parallel(n_jobs=1)]: Done  38 out of  38 | elapsed:    3.4s remaining:    0.0s\n",
      "building tree 39 of 100\n",
      "[Parallel(n_jobs=1)]: Done  39 out of  39 | elapsed:    3.5s remaining:    0.0s\n",
      "building tree 40 of 100\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    3.6s remaining:    0.0s\n",
      "building tree 41 of 100\n",
      "[Parallel(n_jobs=1)]: Done  41 out of  41 | elapsed:    3.7s remaining:    0.0s\n",
      "building tree 42 of 100\n",
      "[Parallel(n_jobs=1)]: Done  42 out of  42 | elapsed:    3.7s remaining:    0.0s\n",
      "building tree 43 of 100\n",
      "[Parallel(n_jobs=1)]: Done  43 out of  43 | elapsed:    3.8s remaining:    0.0s\n",
      "building tree 44 of 100\n",
      "[Parallel(n_jobs=1)]: Done  44 out of  44 | elapsed:    3.9s remaining:    0.0s\n",
      "building tree 45 of 100\n",
      "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed:    4.0s remaining:    0.0s\n",
      "building tree 46 of 100\n",
      "[Parallel(n_jobs=1)]: Done  46 out of  46 | elapsed:    4.1s remaining:    0.0s\n",
      "building tree 47 of 100\n",
      "[Parallel(n_jobs=1)]: Done  47 out of  47 | elapsed:    4.2s remaining:    0.0s\n",
      "building tree 48 of 100\n",
      "[Parallel(n_jobs=1)]: Done  48 out of  48 | elapsed:    4.3s remaining:    0.0s\n",
      "building tree 49 of 100\n",
      "[Parallel(n_jobs=1)]: Done  49 out of  49 | elapsed:    4.4s remaining:    0.0s\n",
      "building tree 50 of 100\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    4.5s remaining:    0.0s\n",
      "building tree 51 of 100\n",
      "[Parallel(n_jobs=1)]: Done  51 out of  51 | elapsed:    4.6s remaining:    0.0s\n",
      "building tree 52 of 100\n",
      "[Parallel(n_jobs=1)]: Done  52 out of  52 | elapsed:    4.6s remaining:    0.0s\n",
      "building tree 53 of 100\n",
      "[Parallel(n_jobs=1)]: Done  53 out of  53 | elapsed:    4.7s remaining:    0.0s\n",
      "building tree 54 of 100\n",
      "[Parallel(n_jobs=1)]: Done  54 out of  54 | elapsed:    4.8s remaining:    0.0s\n",
      "building tree 55 of 100\n",
      "[Parallel(n_jobs=1)]: Done  55 out of  55 | elapsed:    4.9s remaining:    0.0s\n",
      "building tree 56 of 100\n",
      "[Parallel(n_jobs=1)]: Done  56 out of  56 | elapsed:    5.0s remaining:    0.0s\n",
      "building tree 57 of 100\n",
      "[Parallel(n_jobs=1)]: Done  57 out of  57 | elapsed:    5.1s remaining:    0.0s\n",
      "building tree 58 of 100\n",
      "[Parallel(n_jobs=1)]: Done  58 out of  58 | elapsed:    5.2s remaining:    0.0s\n",
      "building tree 59 of 100\n",
      "[Parallel(n_jobs=1)]: Done  59 out of  59 | elapsed:    5.3s remaining:    0.0s\n",
      "building tree 60 of 100\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:    5.4s remaining:    0.0s\n",
      "building tree 61 of 100\n",
      "[Parallel(n_jobs=1)]: Done  61 out of  61 | elapsed:    5.5s remaining:    0.0s\n",
      "building tree 62 of 100\n",
      "[Parallel(n_jobs=1)]: Done  62 out of  62 | elapsed:    5.5s remaining:    0.0s\n",
      "building tree 63 of 100\n",
      "[Parallel(n_jobs=1)]: Done  63 out of  63 | elapsed:    5.6s remaining:    0.0s\n",
      "building tree 64 of 100\n",
      "[Parallel(n_jobs=1)]: Done  64 out of  64 | elapsed:    5.7s remaining:    0.0s\n",
      "building tree 65 of 100\n",
      "[Parallel(n_jobs=1)]: Done  65 out of  65 | elapsed:    5.8s remaining:    0.0s\n",
      "building tree 66 of 100\n",
      "[Parallel(n_jobs=1)]: Done  66 out of  66 | elapsed:    5.9s remaining:    0.0s\n",
      "building tree 67 of 100\n",
      "[Parallel(n_jobs=1)]: Done  67 out of  67 | elapsed:    6.0s remaining:    0.0s\n",
      "building tree 68 of 100\n",
      "[Parallel(n_jobs=1)]: Done  68 out of  68 | elapsed:    6.1s remaining:    0.0s\n",
      "building tree 69 of 100\n",
      "[Parallel(n_jobs=1)]: Done  69 out of  69 | elapsed:    6.2s remaining:    0.0s\n",
      "building tree 70 of 100\n",
      "[Parallel(n_jobs=1)]: Done  70 out of  70 | elapsed:    6.3s remaining:    0.0s\n",
      "building tree 71 of 100\n",
      "[Parallel(n_jobs=1)]: Done  71 out of  71 | elapsed:    6.3s remaining:    0.0s\n",
      "building tree 72 of 100\n",
      "[Parallel(n_jobs=1)]: Done  72 out of  72 | elapsed:    6.4s remaining:    0.0s\n",
      "building tree 73 of 100\n",
      "[Parallel(n_jobs=1)]: Done  73 out of  73 | elapsed:    6.5s remaining:    0.0s\n",
      "building tree 74 of 100\n",
      "[Parallel(n_jobs=1)]: Done  74 out of  74 | elapsed:    6.6s remaining:    0.0s\n",
      "building tree 75 of 100\n",
      "[Parallel(n_jobs=1)]: Done  75 out of  75 | elapsed:    6.7s remaining:    0.0s\n",
      "building tree 76 of 100\n",
      "[Parallel(n_jobs=1)]: Done  76 out of  76 | elapsed:    6.8s remaining:    0.0s\n",
      "building tree 77 of 100\n",
      "[Parallel(n_jobs=1)]: Done  77 out of  77 | elapsed:    6.9s remaining:    0.0s\n",
      "building tree 78 of 100\n",
      "[Parallel(n_jobs=1)]: Done  78 out of  78 | elapsed:    7.0s remaining:    0.0s\n",
      "building tree 79 of 100\n",
      "[Parallel(n_jobs=1)]: Done  79 out of  79 | elapsed:    7.1s remaining:    0.0s\n",
      "building tree 80 of 100\n",
      "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:    7.2s remaining:    0.0s\n",
      "building tree 81 of 100\n",
      "[Parallel(n_jobs=1)]: Done  81 out of  81 | elapsed:    7.2s remaining:    0.0s\n",
      "building tree 82 of 100\n",
      "[Parallel(n_jobs=1)]: Done  82 out of  82 | elapsed:    7.3s remaining:    0.0s\n",
      "building tree 83 of 100\n",
      "[Parallel(n_jobs=1)]: Done  83 out of  83 | elapsed:    7.4s remaining:    0.0s\n",
      "building tree 84 of 100\n",
      "[Parallel(n_jobs=1)]: Done  84 out of  84 | elapsed:    7.5s remaining:    0.0s\n",
      "building tree 85 of 100\n",
      "[Parallel(n_jobs=1)]: Done  85 out of  85 | elapsed:    7.6s remaining:    0.0s\n",
      "building tree 86 of 100\n",
      "[Parallel(n_jobs=1)]: Done  86 out of  86 | elapsed:    7.7s remaining:    0.0s\n",
      "building tree 87 of 100\n",
      "[Parallel(n_jobs=1)]: Done  87 out of  87 | elapsed:    7.8s remaining:    0.0s\n",
      "building tree 88 of 100\n",
      "[Parallel(n_jobs=1)]: Done  88 out of  88 | elapsed:    7.9s remaining:    0.0s\n",
      "building tree 89 of 100\n",
      "[Parallel(n_jobs=1)]: Done  89 out of  89 | elapsed:    8.0s remaining:    0.0s\n",
      "building tree 90 of 100\n",
      "[Parallel(n_jobs=1)]: Done  90 out of  90 | elapsed:    8.0s remaining:    0.0s\n",
      "building tree 91 of 100\n",
      "[Parallel(n_jobs=1)]: Done  91 out of  91 | elapsed:    8.1s remaining:    0.0s\n",
      "building tree 92 of 100\n",
      "[Parallel(n_jobs=1)]: Done  92 out of  92 | elapsed:    8.2s remaining:    0.0s\n",
      "building tree 93 of 100\n",
      "[Parallel(n_jobs=1)]: Done  93 out of  93 | elapsed:    8.3s remaining:    0.0s\n",
      "building tree 94 of 100\n",
      "[Parallel(n_jobs=1)]: Done  94 out of  94 | elapsed:    8.4s remaining:    0.0s\n",
      "building tree 95 of 100\n",
      "[Parallel(n_jobs=1)]: Done  95 out of  95 | elapsed:    8.5s remaining:    0.0s\n",
      "building tree 96 of 100\n",
      "[Parallel(n_jobs=1)]: Done  96 out of  96 | elapsed:    8.6s remaining:    0.0s\n",
      "building tree 97 of 100\n",
      "[Parallel(n_jobs=1)]: Done  97 out of  97 | elapsed:    8.7s remaining:    0.0s\n",
      "building tree 98 of 100\n",
      "[Parallel(n_jobs=1)]: Done  98 out of  98 | elapsed:    8.8s remaining:    0.0s\n",
      "building tree 99 of 100\n",
      "[Parallel(n_jobs=1)]: Done  99 out of  99 | elapsed:    8.8s remaining:    0.0s\n",
      "building tree 100 of 100\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    8.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    8.9s finished\n",
      "[0.04823163 0.07816568 0.03377796 0.03142031 0.01449483 0.00396294\n",
      " 0.0654993  0.0258303  0.02122054 0.04817879 0.43224409 0.03693877\n",
      " 0.01886298 0.14117188] ['Brand' 'Comments' 'Final price' 'Days in stock' 'Days with sales'\n",
      " 'Rating' 'Basic Sale' 'Basic Sale Price' 'Days in stock/sales'\n",
      " 'Comments-Rating' 'Rating-Days-Comments' 'Price difference'\n",
      " 'Min max price diff' 'Rating-Price']\n"
     ]
    }
   ],
   "source": [
    "feature_model = RandomForestRegressor(verbose=999, max_depth=None, random_state=42)\n",
    "\n",
    "feature_model.fit(X_train, y_train)\n",
    "print(feature_model.feature_importances_, feature_model.feature_names_in_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1026,
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     'learning_rate': [0.01, 0.1, 0.2, 0.5, 0.1],\n",
    "#     'min_samples_leaf': [4, 5, 6],\n",
    "#     'loss': ['absolute_error'],\n",
    "#     'l2_regularization': [0.1, 0.2, 0.05],\n",
    "#     'max_bins': [50, 100, 150],\n",
    "#     'max_depth': [None],\n",
    "#     'max_leaf_nodes': [None, 5, 6, 8],\n",
    "#     'validation_fraction': [0.01, 0.1, 0.2]\n",
    "# }\n",
    "#\n",
    "# boost_model = GridSearchCV(\n",
    "#     HistGradientBoostingRegressor(max_iter=1000, max_bins=100, random_state=42),\n",
    "#     params, scoring='neg_mean_absolute_error', n_jobs=-1, verbose=999\n",
    "# )\n",
    "#\n",
    "# boost_model.fit(X_train, y_train)\n",
    "# # learning_rate=0.01, verbose=999, max_depth=None, min_samples_leaf=30, max_iter=1000, loss='absolute_error', l2_regularization=0.02, max_bins=100, validation_fraction=0.02, random_state=42\n",
    "#\n",
    "# # learning_rate=0.01, verbose=999, max_depth=None, min_samples_leaf=4, max_iter=1000, loss='absolute_error', l2_regularization=0.2, max_bins=100, validation_fraction=0.01, random_state=42\n",
    "#\n",
    "# # 'l2_regularization': 0.05,\n",
    "# #  'learning_rate': 0.01,\n",
    "# #  'loss': 'absolute_error',\n",
    "# #  'max_bins': 150,\n",
    "# #  'max_depth': None,\n",
    "# #  'max_leaf_nodes': None,\n",
    "# #  'min_samples_leaf': 6,\n",
    "# #  'validation_fraction': 0.2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1027,
   "outputs": [],
   "source": [
    "# boost_model.best_params_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1028,
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "#\n",
    "# pipeline = make_pipeline(\n",
    "#     PolynomialFeatures(degree=3),\n",
    "#     SelectKBest(mutual_info_regression, k=10)\n",
    "# )\n",
    "#\n",
    "# poly_features = pipeline.fit_transform(X_train, y_train)\n",
    "# poly_features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1029,
   "outputs": [],
   "source": [
    "# import xgboost as xgb\n",
    "#\n",
    "# xgb_model = xgb.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "#        colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
    "#        importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
    "#        max_depth=3, min_child_weight=1, missing=True, n_estimators=100,\n",
    "#        n_jobs=1,random_state=0,\n",
    "#        reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
    "#        silent=None, subsample=1, verbosity=1)\n",
    "# xgb_model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1030,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binning 0.002 GB of training data: 0.021 s\n",
      "Binning 0.000 GB of validation data: 0.000 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/10000] 1 tree, 31 leaves, max depth = 9, train loss: 39.56109, val loss: 29.02310, in 0.010s\n",
      "[2/10000] 1 tree, 31 leaves, max depth = 9, train loss: 39.47259, val loss: 28.93656, in 0.009s\n",
      "[3/10000] 1 tree, 31 leaves, max depth = 9, train loss: 39.38481, val loss: 28.85090, in 0.014s\n",
      "[4/10000] 1 tree, 31 leaves, max depth = 10, train loss: 39.29800, val loss: 28.76615, in 0.005s\n",
      "[5/10000] 1 tree, 31 leaves, max depth = 9, train loss: 39.21207, val loss: 28.68202, in 0.015s\n",
      "[6/10000] 1 tree, 31 leaves, max depth = 9, train loss: 39.12736, val loss: 28.59963, in 0.005s\n",
      "[7/10000] 1 tree, 31 leaves, max depth = 9, train loss: 39.03590, val loss: 28.51315, in 0.006s\n",
      "[8/10000] 1 tree, 31 leaves, max depth = 9, train loss: 38.94541, val loss: 28.42753, in 0.006s\n",
      "[9/10000] 1 tree, 31 leaves, max depth = 9, train loss: 38.85587, val loss: 28.34277, in 0.005s\n",
      "[10/10000] 1 tree, 31 leaves, max depth = 9, train loss: 38.76716, val loss: 28.25891, in 0.006s\n",
      "[11/10000] 1 tree, 31 leaves, max depth = 9, train loss: 38.68012, val loss: 28.17606, in 0.016s\n",
      "[12/10000] 1 tree, 31 leaves, max depth = 7, train loss: 38.58724, val loss: 28.08748, in 0.006s\n",
      "[13/10000] 1 tree, 31 leaves, max depth = 7, train loss: 38.49531, val loss: 28.00017, in 0.005s\n",
      "[14/10000] 1 tree, 31 leaves, max depth = 7, train loss: 38.40431, val loss: 27.91374, in 0.007s\n",
      "[15/10000] 1 tree, 31 leaves, max depth = 7, train loss: 38.31421, val loss: 27.82817, in 0.009s\n",
      "[16/10000] 1 tree, 31 leaves, max depth = 7, train loss: 38.22688, val loss: 27.74605, in 0.017s\n",
      "[17/10000] 1 tree, 31 leaves, max depth = 10, train loss: 38.13059, val loss: 27.65354, in 0.006s\n",
      "[18/10000] 1 tree, 31 leaves, max depth = 10, train loss: 38.03519, val loss: 27.56141, in 0.005s\n",
      "[19/10000] 1 tree, 31 leaves, max depth = 10, train loss: 37.94105, val loss: 27.47111, in 0.005s\n",
      "[20/10000] 1 tree, 31 leaves, max depth = 10, train loss: 37.84179, val loss: 27.37314, in 0.007s\n",
      "[21/10000] 1 tree, 31 leaves, max depth = 10, train loss: 37.74356, val loss: 27.27596, in 0.009s\n",
      "[22/10000] 1 tree, 31 leaves, max depth = 10, train loss: 37.64638, val loss: 27.17976, in 0.007s\n",
      "[23/10000] 1 tree, 31 leaves, max depth = 10, train loss: 37.55096, val loss: 27.08811, in 0.005s\n",
      "[24/10000] 1 tree, 31 leaves, max depth = 8, train loss: 37.45608, val loss: 26.99622, in 0.005s\n",
      "[25/10000] 1 tree, 31 leaves, max depth = 7, train loss: 37.36221, val loss: 26.90527, in 0.006s\n",
      "[26/10000] 1 tree, 31 leaves, max depth = 7, train loss: 37.27083, val loss: 26.81541, in 0.005s\n",
      "[27/10000] 1 tree, 31 leaves, max depth = 9, train loss: 37.17384, val loss: 26.71686, in 0.020s\n",
      "[28/10000] 1 tree, 31 leaves, max depth = 9, train loss: 37.07782, val loss: 26.61929, in 0.006s\n",
      "[29/10000] 1 tree, 31 leaves, max depth = 9, train loss: 36.98405, val loss: 26.52270, in 0.010s\n",
      "[30/10000] 1 tree, 31 leaves, max depth = 7, train loss: 36.88536, val loss: 26.41677, in 0.005s\n",
      "[31/10000] 1 tree, 31 leaves, max depth = 8, train loss: 36.78783, val loss: 26.31195, in 0.005s\n",
      "[32/10000] 1 tree, 31 leaves, max depth = 7, train loss: 36.68494, val loss: 26.20540, in 0.006s\n",
      "[33/10000] 1 tree, 31 leaves, max depth = 7, train loss: 36.58308, val loss: 26.09996, in 0.006s\n",
      "[34/10000] 1 tree, 31 leaves, max depth = 7, train loss: 36.48338, val loss: 26.00143, in 0.006s\n",
      "[35/10000] 1 tree, 31 leaves, max depth = 7, train loss: 36.39628, val loss: 25.92174, in 0.006s\n",
      "[36/10000] 1 tree, 31 leaves, max depth = 8, train loss: 36.29724, val loss: 25.82010, in 0.005s\n",
      "[37/10000] 1 tree, 31 leaves, max depth = 7, train loss: 36.20070, val loss: 25.72296, in 0.008s\n",
      "[38/10000] 1 tree, 31 leaves, max depth = 8, train loss: 36.10545, val loss: 25.62553, in 0.005s\n",
      "[39/10000] 1 tree, 31 leaves, max depth = 8, train loss: 36.00541, val loss: 25.51969, in 0.005s\n",
      "[40/10000] 1 tree, 31 leaves, max depth = 8, train loss: 35.89970, val loss: 25.40518, in 0.005s\n",
      "[41/10000] 1 tree, 31 leaves, max depth = 7, train loss: 35.79652, val loss: 25.29255, in 0.007s\n",
      "[42/10000] 1 tree, 31 leaves, max depth = 7, train loss: 35.69170, val loss: 25.18092, in 0.006s\n",
      "[43/10000] 1 tree, 31 leaves, max depth = 9, train loss: 35.58785, val loss: 25.06999, in 0.006s\n",
      "[44/10000] 1 tree, 31 leaves, max depth = 7, train loss: 35.48148, val loss: 24.95808, in 0.011s\n",
      "[45/10000] 1 tree, 31 leaves, max depth = 8, train loss: 35.38105, val loss: 24.85114, in 0.006s\n",
      "[46/10000] 1 tree, 31 leaves, max depth = 8, train loss: 35.27921, val loss: 24.74428, in 0.005s\n",
      "[47/10000] 1 tree, 31 leaves, max depth = 7, train loss: 35.17529, val loss: 24.62897, in 0.010s\n",
      "[48/10000] 1 tree, 31 leaves, max depth = 7, train loss: 35.07343, val loss: 24.51967, in 0.005s\n",
      "[49/10000] 1 tree, 31 leaves, max depth = 7, train loss: 34.96762, val loss: 24.40937, in 0.006s\n",
      "[50/10000] 1 tree, 31 leaves, max depth = 7, train loss: 34.85972, val loss: 24.30010, in 0.008s\n",
      "[51/10000] 1 tree, 31 leaves, max depth = 9, train loss: 34.75676, val loss: 24.20190, in 0.006s\n",
      "[52/10000] 1 tree, 31 leaves, max depth = 9, train loss: 34.64773, val loss: 24.09547, in 0.006s\n",
      "[53/10000] 1 tree, 31 leaves, max depth = 8, train loss: 34.54080, val loss: 23.99014, in 0.005s\n",
      "[54/10000] 1 tree, 31 leaves, max depth = 9, train loss: 34.43948, val loss: 23.89109, in 0.006s\n",
      "[55/10000] 1 tree, 31 leaves, max depth = 9, train loss: 34.33786, val loss: 23.79000, in 0.007s\n",
      "[56/10000] 1 tree, 31 leaves, max depth = 8, train loss: 34.22756, val loss: 23.68400, in 0.006s\n",
      "[57/10000] 1 tree, 31 leaves, max depth = 8, train loss: 34.11704, val loss: 23.58159, in 0.006s\n",
      "[58/10000] 1 tree, 31 leaves, max depth = 8, train loss: 34.00911, val loss: 23.47981, in 0.010s\n",
      "[59/10000] 1 tree, 31 leaves, max depth = 8, train loss: 33.90866, val loss: 23.38073, in 0.006s\n",
      "[60/10000] 1 tree, 31 leaves, max depth = 8, train loss: 33.79945, val loss: 23.27888, in 0.006s\n",
      "[61/10000] 1 tree, 31 leaves, max depth = 8, train loss: 33.68816, val loss: 23.17162, in 0.005s\n",
      "[62/10000] 1 tree, 31 leaves, max depth = 7, train loss: 33.57618, val loss: 23.06689, in 0.005s\n",
      "[63/10000] 1 tree, 31 leaves, max depth = 7, train loss: 33.46443, val loss: 22.96996, in 0.080s\n",
      "[64/10000] 1 tree, 31 leaves, max depth = 8, train loss: 33.36503, val loss: 22.87325, in 0.014s\n",
      "[65/10000] 1 tree, 31 leaves, max depth = 8, train loss: 33.25490, val loss: 22.76697, in 0.044s\n",
      "[66/10000] 1 tree, 31 leaves, max depth = 7, train loss: 33.14701, val loss: 22.67051, in 0.008s\n",
      "[67/10000] 1 tree, 31 leaves, max depth = 8, train loss: 33.03520, val loss: 22.55647, in 0.020s\n",
      "[68/10000] 1 tree, 31 leaves, max depth = 7, train loss: 32.92529, val loss: 22.44343, in 0.009s\n",
      "[69/10000] 1 tree, 31 leaves, max depth = 7, train loss: 32.82068, val loss: 22.33655, in 0.006s\n",
      "[70/10000] 1 tree, 31 leaves, max depth = 8, train loss: 32.70479, val loss: 22.23250, in 0.006s\n",
      "[71/10000] 1 tree, 31 leaves, max depth = 9, train loss: 32.59089, val loss: 22.12965, in 0.005s\n",
      "[72/10000] 1 tree, 31 leaves, max depth = 8, train loss: 32.47869, val loss: 22.02820, in 0.006s\n",
      "[73/10000] 1 tree, 31 leaves, max depth = 7, train loss: 32.36707, val loss: 21.93112, in 0.010s\n",
      "[74/10000] 1 tree, 31 leaves, max depth = 7, train loss: 32.25554, val loss: 21.81046, in 0.006s\n",
      "[75/10000] 1 tree, 31 leaves, max depth = 7, train loss: 32.14671, val loss: 21.70991, in 0.006s\n",
      "[76/10000] 1 tree, 31 leaves, max depth = 9, train loss: 32.03532, val loss: 21.61194, in 0.006s\n",
      "[77/10000] 1 tree, 31 leaves, max depth = 8, train loss: 31.92598, val loss: 21.51278, in 0.007s\n",
      "[78/10000] 1 tree, 31 leaves, max depth = 7, train loss: 31.81668, val loss: 21.41795, in 0.006s\n",
      "[79/10000] 1 tree, 31 leaves, max depth = 7, train loss: 31.70542, val loss: 21.31734, in 0.005s\n",
      "[80/10000] 1 tree, 31 leaves, max depth = 7, train loss: 31.60544, val loss: 21.22476, in 0.015s\n",
      "[81/10000] 1 tree, 31 leaves, max depth = 9, train loss: 31.49721, val loss: 21.12640, in 0.006s\n",
      "[82/10000] 1 tree, 31 leaves, max depth = 9, train loss: 31.38726, val loss: 21.03231, in 0.007s\n",
      "[83/10000] 1 tree, 31 leaves, max depth = 10, train loss: 31.27822, val loss: 20.93995, in 0.006s\n",
      "[84/10000] 1 tree, 31 leaves, max depth = 10, train loss: 31.17171, val loss: 20.85146, in 0.006s\n",
      "[85/10000] 1 tree, 31 leaves, max depth = 9, train loss: 31.07801, val loss: 20.77655, in 0.006s\n",
      "[86/10000] 1 tree, 31 leaves, max depth = 10, train loss: 30.97972, val loss: 20.68484, in 0.008s\n",
      "[87/10000] 1 tree, 31 leaves, max depth = 10, train loss: 30.88394, val loss: 20.60620, in 0.006s\n",
      "[88/10000] 1 tree, 31 leaves, max depth = 7, train loss: 30.77816, val loss: 20.51544, in 0.005s\n",
      "[89/10000] 1 tree, 31 leaves, max depth = 9, train loss: 30.67010, val loss: 20.41617, in 0.006s\n",
      "[90/10000] 1 tree, 31 leaves, max depth = 9, train loss: 30.55534, val loss: 20.30426, in 0.005s\n",
      "[91/10000] 1 tree, 31 leaves, max depth = 9, train loss: 30.44258, val loss: 20.19348, in 0.006s\n",
      "[92/10000] 1 tree, 31 leaves, max depth = 9, train loss: 30.33008, val loss: 20.09436, in 0.061s\n",
      "[93/10000] 1 tree, 31 leaves, max depth = 9, train loss: 30.21668, val loss: 20.01087, in 0.032s\n",
      "[94/10000] 1 tree, 31 leaves, max depth = 9, train loss: 30.10815, val loss: 19.93010, in 0.006s\n",
      "[95/10000] 1 tree, 31 leaves, max depth = 9, train loss: 29.99262, val loss: 19.84029, in 0.006s\n",
      "[96/10000] 1 tree, 31 leaves, max depth = 9, train loss: 29.90355, val loss: 19.76776, in 0.006s\n",
      "[97/10000] 1 tree, 31 leaves, max depth = 9, train loss: 29.80563, val loss: 19.70749, in 0.009s\n",
      "[98/10000] 1 tree, 31 leaves, max depth = 9, train loss: 29.72008, val loss: 19.63873, in 0.005s\n",
      "[99/10000] 1 tree, 31 leaves, max depth = 9, train loss: 29.62597, val loss: 19.56147, in 0.006s\n",
      "[100/10000] 1 tree, 31 leaves, max depth = 7, train loss: 29.53407, val loss: 19.49014, in 0.006s\n",
      "[101/10000] 1 tree, 31 leaves, max depth = 9, train loss: 29.42759, val loss: 19.41494, in 0.006s\n",
      "[102/10000] 1 tree, 31 leaves, max depth = 9, train loss: 29.32367, val loss: 19.34980, in 0.006s\n",
      "[103/10000] 1 tree, 31 leaves, max depth = 9, train loss: 29.22138, val loss: 19.27333, in 0.006s\n",
      "[104/10000] 1 tree, 31 leaves, max depth = 8, train loss: 29.11915, val loss: 19.17659, in 0.006s\n",
      "[105/10000] 1 tree, 31 leaves, max depth = 8, train loss: 29.01407, val loss: 19.09192, in 0.008s\n",
      "[106/10000] 1 tree, 31 leaves, max depth = 7, train loss: 28.90986, val loss: 19.00880, in 0.006s\n",
      "[107/10000] 1 tree, 31 leaves, max depth = 8, train loss: 28.81294, val loss: 18.94715, in 0.006s\n",
      "[108/10000] 1 tree, 31 leaves, max depth = 8, train loss: 28.71832, val loss: 18.88581, in 0.006s\n",
      "[109/10000] 1 tree, 31 leaves, max depth = 13, train loss: 28.62417, val loss: 18.81723, in 0.005s\n",
      "[110/10000] 1 tree, 31 leaves, max depth = 8, train loss: 28.53025, val loss: 18.74518, in 0.006s\n",
      "[111/10000] 1 tree, 31 leaves, max depth = 10, train loss: 28.43719, val loss: 18.67453, in 0.006s\n",
      "[112/10000] 1 tree, 31 leaves, max depth = 9, train loss: 28.33341, val loss: 18.57420, in 0.009s\n",
      "[113/10000] 1 tree, 31 leaves, max depth = 9, train loss: 28.25073, val loss: 18.50331, in 0.006s\n",
      "[114/10000] 1 tree, 31 leaves, max depth = 8, train loss: 28.14566, val loss: 18.43264, in 0.006s\n",
      "[115/10000] 1 tree, 31 leaves, max depth = 8, train loss: 28.04647, val loss: 18.33501, in 0.006s\n",
      "[116/10000] 1 tree, 31 leaves, max depth = 8, train loss: 27.94726, val loss: 18.24269, in 0.007s\n",
      "[117/10000] 1 tree, 31 leaves, max depth = 9, train loss: 27.85026, val loss: 18.14736, in 0.006s\n",
      "[118/10000] 1 tree, 31 leaves, max depth = 8, train loss: 27.75678, val loss: 18.05242, in 0.009s\n",
      "[119/10000] 1 tree, 31 leaves, max depth = 8, train loss: 27.65753, val loss: 17.98801, in 0.006s\n",
      "[120/10000] 1 tree, 31 leaves, max depth = 8, train loss: 27.56071, val loss: 17.92154, in 0.006s\n",
      "[121/10000] 1 tree, 31 leaves, max depth = 8, train loss: 27.46428, val loss: 17.85247, in 0.006s\n",
      "[122/10000] 1 tree, 31 leaves, max depth = 9, train loss: 27.36289, val loss: 17.74958, in 0.005s\n",
      "[123/10000] 1 tree, 31 leaves, max depth = 8, train loss: 27.26520, val loss: 17.64744, in 0.005s\n",
      "[124/10000] 1 tree, 31 leaves, max depth = 8, train loss: 27.19414, val loss: 17.59909, in 0.007s\n",
      "[125/10000] 1 tree, 31 leaves, max depth = 7, train loss: 27.11692, val loss: 17.53750, in 0.005s\n",
      "[126/10000] 1 tree, 31 leaves, max depth = 7, train loss: 27.02089, val loss: 17.44085, in 0.005s\n",
      "[127/10000] 1 tree, 31 leaves, max depth = 8, train loss: 26.93877, val loss: 17.35296, in 0.009s\n",
      "[128/10000] 1 tree, 31 leaves, max depth = 8, train loss: 26.87015, val loss: 17.29210, in 0.006s\n",
      "[129/10000] 1 tree, 31 leaves, max depth = 9, train loss: 26.80501, val loss: 17.23641, in 0.006s\n",
      "[130/10000] 1 tree, 31 leaves, max depth = 8, train loss: 26.73507, val loss: 17.21023, in 0.006s\n",
      "[131/10000] 1 tree, 31 leaves, max depth = 7, train loss: 26.65576, val loss: 17.13820, in 0.006s\n",
      "[132/10000] 1 tree, 31 leaves, max depth = 8, train loss: 26.57261, val loss: 17.08007, in 0.006s\n",
      "[133/10000] 1 tree, 31 leaves, max depth = 9, train loss: 26.51424, val loss: 17.03740, in 0.005s\n",
      "[134/10000] 1 tree, 31 leaves, max depth = 9, train loss: 26.45763, val loss: 16.98744, in 0.009s\n",
      "[135/10000] 1 tree, 31 leaves, max depth = 8, train loss: 26.38087, val loss: 16.92554, in 0.006s\n",
      "[136/10000] 1 tree, 31 leaves, max depth = 9, train loss: 26.32615, val loss: 16.89102, in 0.005s\n",
      "[137/10000] 1 tree, 31 leaves, max depth = 8, train loss: 26.25372, val loss: 16.83578, in 0.005s\n",
      "[138/10000] 1 tree, 31 leaves, max depth = 8, train loss: 26.18754, val loss: 16.77615, in 0.006s\n",
      "[139/10000] 1 tree, 31 leaves, max depth = 7, train loss: 26.12041, val loss: 16.71838, in 0.006s\n",
      "[140/10000] 1 tree, 31 leaves, max depth = 7, train loss: 26.04922, val loss: 16.65319, in 0.007s\n",
      "[141/10000] 1 tree, 31 leaves, max depth = 9, train loss: 25.99288, val loss: 16.60607, in 0.008s\n",
      "[142/10000] 1 tree, 31 leaves, max depth = 8, train loss: 25.93216, val loss: 16.53449, in 0.006s\n",
      "[143/10000] 1 tree, 31 leaves, max depth = 9, train loss: 25.86714, val loss: 16.49055, in 0.006s\n",
      "[144/10000] 1 tree, 31 leaves, max depth = 8, train loss: 25.82061, val loss: 16.45061, in 0.039s\n",
      "[145/10000] 1 tree, 31 leaves, max depth = 7, train loss: 25.75090, val loss: 16.38190, in 0.006s\n",
      "[146/10000] 1 tree, 31 leaves, max depth = 8, train loss: 25.68077, val loss: 16.30638, in 0.006s\n",
      "[147/10000] 1 tree, 31 leaves, max depth = 8, train loss: 25.61938, val loss: 16.26120, in 0.006s\n",
      "[148/10000] 1 tree, 31 leaves, max depth = 10, train loss: 25.54947, val loss: 16.19454, in 0.009s\n",
      "[149/10000] 1 tree, 31 leaves, max depth = 9, train loss: 25.49487, val loss: 16.15084, in 0.006s\n",
      "[150/10000] 1 tree, 31 leaves, max depth = 8, train loss: 25.43506, val loss: 16.10683, in 0.005s\n",
      "[151/10000] 1 tree, 31 leaves, max depth = 8, train loss: 25.37369, val loss: 16.08546, in 0.005s\n",
      "[152/10000] 1 tree, 31 leaves, max depth = 8, train loss: 25.31705, val loss: 16.04475, in 0.006s\n",
      "[153/10000] 1 tree, 31 leaves, max depth = 9, train loss: 25.25903, val loss: 16.02482, in 0.006s\n",
      "[154/10000] 1 tree, 31 leaves, max depth = 8, train loss: 25.20177, val loss: 15.99999, in 0.005s\n",
      "[155/10000] 1 tree, 31 leaves, max depth = 9, train loss: 25.14551, val loss: 15.98149, in 0.010s\n",
      "[156/10000] 1 tree, 31 leaves, max depth = 10, train loss: 25.08433, val loss: 15.95670, in 0.007s\n",
      "[157/10000] 1 tree, 31 leaves, max depth = 8, train loss: 25.02759, val loss: 15.88538, in 0.006s\n",
      "[158/10000] 1 tree, 31 leaves, max depth = 8, train loss: 24.97799, val loss: 15.86914, in 0.006s\n",
      "[159/10000] 1 tree, 31 leaves, max depth = 8, train loss: 24.91810, val loss: 15.81267, in 0.007s\n",
      "[160/10000] 1 tree, 31 leaves, max depth = 10, train loss: 24.86323, val loss: 15.76717, in 0.005s\n",
      "[161/10000] 1 tree, 31 leaves, max depth = 11, train loss: 24.80702, val loss: 15.71713, in 0.006s\n",
      "[162/10000] 1 tree, 31 leaves, max depth = 8, train loss: 24.76497, val loss: 15.67238, in 0.009s\n",
      "[163/10000] 1 tree, 31 leaves, max depth = 9, train loss: 24.71262, val loss: 15.62579, in 0.006s\n",
      "[164/10000] 1 tree, 31 leaves, max depth = 7, train loss: 24.66092, val loss: 15.56035, in 0.005s\n",
      "[165/10000] 1 tree, 31 leaves, max depth = 8, train loss: 24.61516, val loss: 15.52982, in 0.006s\n",
      "[166/10000] 1 tree, 31 leaves, max depth = 7, train loss: 24.57106, val loss: 15.47429, in 0.007s\n",
      "[167/10000] 1 tree, 31 leaves, max depth = 8, train loss: 24.51862, val loss: 15.43441, in 0.006s\n",
      "[168/10000] 1 tree, 31 leaves, max depth = 9, train loss: 24.47328, val loss: 15.37156, in 0.005s\n",
      "[169/10000] 1 tree, 31 leaves, max depth = 8, train loss: 24.42793, val loss: 15.30927, in 0.009s\n",
      "[170/10000] 1 tree, 31 leaves, max depth = 8, train loss: 24.38147, val loss: 15.24996, in 0.006s\n",
      "[171/10000] 1 tree, 31 leaves, max depth = 8, train loss: 24.33294, val loss: 15.21084, in 0.006s\n",
      "[172/10000] 1 tree, 31 leaves, max depth = 8, train loss: 24.28460, val loss: 15.16070, in 0.006s\n",
      "[173/10000] 1 tree, 31 leaves, max depth = 9, train loss: 24.23911, val loss: 15.15925, in 0.007s\n",
      "[174/10000] 1 tree, 31 leaves, max depth = 8, train loss: 24.18936, val loss: 15.12100, in 0.006s\n",
      "[175/10000] 1 tree, 31 leaves, max depth = 10, train loss: 24.14148, val loss: 15.09758, in 0.006s\n",
      "[176/10000] 1 tree, 31 leaves, max depth = 8, train loss: 24.09064, val loss: 15.08828, in 0.009s\n",
      "[177/10000] 1 tree, 31 leaves, max depth = 8, train loss: 24.05145, val loss: 15.07654, in 0.006s\n",
      "[178/10000] 1 tree, 31 leaves, max depth = 8, train loss: 24.02403, val loss: 15.05073, in 0.006s\n",
      "[179/10000] 1 tree, 31 leaves, max depth = 10, train loss: 23.97526, val loss: 15.01385, in 0.021s\n",
      "[180/10000] 1 tree, 31 leaves, max depth = 8, train loss: 23.93072, val loss: 14.99965, in 0.006s\n",
      "[181/10000] 1 tree, 31 leaves, max depth = 10, train loss: 23.90238, val loss: 14.98213, in 0.009s\n",
      "[182/10000] 1 tree, 31 leaves, max depth = 10, train loss: 23.86195, val loss: 14.95673, in 0.005s\n",
      "[183/10000] 1 tree, 31 leaves, max depth = 8, train loss: 23.81917, val loss: 14.94035, in 0.006s\n",
      "[184/10000] 1 tree, 31 leaves, max depth = 8, train loss: 23.78233, val loss: 14.91604, in 0.006s\n",
      "[185/10000] 1 tree, 31 leaves, max depth = 8, train loss: 23.74143, val loss: 14.90143, in 0.006s\n",
      "[186/10000] 1 tree, 31 leaves, max depth = 8, train loss: 23.70221, val loss: 14.86909, in 0.009s\n",
      "[187/10000] 1 tree, 31 leaves, max depth = 9, train loss: 23.65724, val loss: 14.83911, in 0.006s\n",
      "[188/10000] 1 tree, 31 leaves, max depth = 8, train loss: 23.61907, val loss: 14.80749, in 0.009s\n",
      "[189/10000] 1 tree, 31 leaves, max depth = 8, train loss: 23.58195, val loss: 14.77616, in 0.006s\n",
      "[190/10000] 1 tree, 31 leaves, max depth = 9, train loss: 23.54190, val loss: 14.74576, in 0.006s\n",
      "[191/10000] 1 tree, 31 leaves, max depth = 8, train loss: 23.50138, val loss: 14.71201, in 0.005s\n",
      "[192/10000] 1 tree, 31 leaves, max depth = 11, train loss: 23.46747, val loss: 14.68471, in 0.006s\n",
      "[193/10000] 1 tree, 31 leaves, max depth = 10, train loss: 23.42533, val loss: 14.65340, in 0.005s\n",
      "[194/10000] 1 tree, 31 leaves, max depth = 9, train loss: 23.39495, val loss: 14.62779, in 0.007s\n",
      "[195/10000] 1 tree, 31 leaves, max depth = 9, train loss: 23.36334, val loss: 14.60446, in 0.010s\n",
      "[196/10000] 1 tree, 31 leaves, max depth = 10, train loss: 23.31775, val loss: 14.54515, in 0.006s\n",
      "[197/10000] 1 tree, 31 leaves, max depth = 10, train loss: 23.27916, val loss: 14.51648, in 0.005s\n",
      "[198/10000] 1 tree, 31 leaves, max depth = 10, train loss: 23.24754, val loss: 14.48898, in 0.006s\n",
      "[199/10000] 1 tree, 31 leaves, max depth = 10, train loss: 23.20124, val loss: 14.42809, in 0.005s\n",
      "[200/10000] 1 tree, 31 leaves, max depth = 9, train loss: 23.18553, val loss: 14.42528, in 0.005s\n",
      "[201/10000] 1 tree, 31 leaves, max depth = 8, train loss: 23.17056, val loss: 14.42314, in 0.006s\n",
      "[202/10000] 1 tree, 31 leaves, max depth = 8, train loss: 23.10889, val loss: 14.34845, in 0.013s\n",
      "[203/10000] 1 tree, 31 leaves, max depth = 9, train loss: 23.07882, val loss: 14.34258, in 0.006s\n",
      "[204/10000] 1 tree, 31 leaves, max depth = 8, train loss: 23.04715, val loss: 14.31503, in 0.005s\n",
      "[205/10000] 1 tree, 31 leaves, max depth = 9, train loss: 23.01028, val loss: 14.25849, in 0.005s\n",
      "[206/10000] 1 tree, 31 leaves, max depth = 10, train loss: 22.98139, val loss: 14.24873, in 0.006s\n",
      "[207/10000] 1 tree, 31 leaves, max depth = 11, train loss: 22.96468, val loss: 14.25354, in 0.006s\n",
      "[208/10000] 1 tree, 31 leaves, max depth = 10, train loss: 22.93721, val loss: 14.23845, in 0.007s\n",
      "[209/10000] 1 tree, 31 leaves, max depth = 10, train loss: 22.91019, val loss: 14.22352, in 0.012s\n",
      "[210/10000] 1 tree, 31 leaves, max depth = 9, train loss: 22.87931, val loss: 14.19486, in 0.006s\n",
      "[211/10000] 1 tree, 31 leaves, max depth = 9, train loss: 22.85378, val loss: 14.17309, in 0.005s\n",
      "[212/10000] 1 tree, 31 leaves, max depth = 12, train loss: 22.82891, val loss: 14.15195, in 0.006s\n",
      "[213/10000] 1 tree, 31 leaves, max depth = 11, train loss: 22.80209, val loss: 14.12653, in 0.005s\n",
      "[214/10000] 1 tree, 31 leaves, max depth = 8, train loss: 22.78266, val loss: 14.10577, in 0.011s\n",
      "[215/10000] 1 tree, 31 leaves, max depth = 9, train loss: 22.73418, val loss: 14.03342, in 0.006s\n",
      "[216/10000] 1 tree, 31 leaves, max depth = 9, train loss: 22.70788, val loss: 14.01619, in 0.006s\n",
      "[217/10000] 1 tree, 31 leaves, max depth = 9, train loss: 22.66715, val loss: 14.00029, in 0.010s\n",
      "[218/10000] 1 tree, 31 leaves, max depth = 11, train loss: 22.62745, val loss: 13.98468, in 0.006s\n",
      "[219/10000] 1 tree, 31 leaves, max depth = 9, train loss: 22.60264, val loss: 13.96816, in 0.006s\n",
      "[220/10000] 1 tree, 31 leaves, max depth = 11, train loss: 22.56382, val loss: 13.94940, in 0.006s\n",
      "[221/10000] 1 tree, 31 leaves, max depth = 9, train loss: 22.52651, val loss: 13.93029, in 0.006s\n",
      "[222/10000] 1 tree, 31 leaves, max depth = 10, train loss: 22.48905, val loss: 13.91159, in 0.008s\n",
      "[223/10000] 1 tree, 31 leaves, max depth = 9, train loss: 22.46581, val loss: 13.89430, in 0.010s\n",
      "[224/10000] 1 tree, 31 leaves, max depth = 9, train loss: 22.42878, val loss: 13.87470, in 0.007s\n",
      "[225/10000] 1 tree, 31 leaves, max depth = 8, train loss: 22.40842, val loss: 13.85748, in 0.006s\n",
      "[226/10000] 1 tree, 31 leaves, max depth = 9, train loss: 22.38945, val loss: 13.82422, in 0.005s\n",
      "[227/10000] 1 tree, 31 leaves, max depth = 9, train loss: 22.36378, val loss: 13.79644, in 0.052s\n",
      "[228/10000] 1 tree, 31 leaves, max depth = 9, train loss: 22.34536, val loss: 13.76370, in 0.008s\n",
      "[229/10000] 1 tree, 31 leaves, max depth = 9, train loss: 22.32685, val loss: 13.75868, in 0.005s\n",
      "[230/10000] 1 tree, 31 leaves, max depth = 11, train loss: 22.31342, val loss: 13.76593, in 0.006s\n",
      "[231/10000] 1 tree, 31 leaves, max depth = 9, train loss: 22.28630, val loss: 13.73651, in 0.006s\n",
      "[232/10000] 1 tree, 31 leaves, max depth = 8, train loss: 22.25789, val loss: 13.70375, in 0.007s\n",
      "[233/10000] 1 tree, 31 leaves, max depth = 11, train loss: 22.23836, val loss: 13.68241, in 0.006s\n",
      "[234/10000] 1 tree, 31 leaves, max depth = 8, train loss: 22.22611, val loss: 13.66411, in 0.005s\n",
      "[235/10000] 1 tree, 31 leaves, max depth = 10, train loss: 22.21433, val loss: 13.66816, in 0.010s\n",
      "[236/10000] 1 tree, 31 leaves, max depth = 10, train loss: 22.20284, val loss: 13.67182, in 0.006s\n",
      "[237/10000] 1 tree, 31 leaves, max depth = 9, train loss: 22.17343, val loss: 13.63520, in 0.005s\n",
      "[238/10000] 1 tree, 31 leaves, max depth = 11, train loss: 22.15577, val loss: 13.63617, in 0.005s\n",
      "[239/10000] 1 tree, 31 leaves, max depth = 10, train loss: 22.12874, val loss: 13.60003, in 0.007s\n",
      "[240/10000] 1 tree, 31 leaves, max depth = 9, train loss: 22.10224, val loss: 13.56452, in 0.006s\n",
      "[241/10000] 1 tree, 31 leaves, max depth = 11, train loss: 22.07583, val loss: 13.52876, in 0.006s\n",
      "[242/10000] 1 tree, 31 leaves, max depth = 9, train loss: 22.05772, val loss: 13.51711, in 0.010s\n",
      "[243/10000] 1 tree, 31 leaves, max depth = 11, train loss: 22.03481, val loss: 13.49163, in 0.006s\n",
      "[244/10000] 1 tree, 31 leaves, max depth = 11, train loss: 22.01101, val loss: 13.46551, in 0.005s\n",
      "[245/10000] 1 tree, 31 leaves, max depth = 10, train loss: 21.98908, val loss: 13.45920, in 0.007s\n",
      "[246/10000] 1 tree, 31 leaves, max depth = 10, train loss: 21.96806, val loss: 13.45296, in 0.006s\n",
      "[247/10000] 1 tree, 31 leaves, max depth = 11, train loss: 21.94961, val loss: 13.44596, in 0.006s\n",
      "[248/10000] 1 tree, 31 leaves, max depth = 9, train loss: 21.92691, val loss: 13.42215, in 0.010s\n",
      "[249/10000] 1 tree, 31 leaves, max depth = 9, train loss: 21.91183, val loss: 13.40258, in 0.005s\n",
      "[250/10000] 1 tree, 31 leaves, max depth = 9, train loss: 21.89878, val loss: 13.38779, in 0.006s\n",
      "[251/10000] 1 tree, 31 leaves, max depth = 10, train loss: 21.87656, val loss: 13.36429, in 0.006s\n",
      "[252/10000] 1 tree, 31 leaves, max depth = 10, train loss: 21.85606, val loss: 13.34899, in 0.006s\n",
      "[253/10000] 1 tree, 31 leaves, max depth = 11, train loss: 21.82190, val loss: 13.35185, in 0.006s\n",
      "[254/10000] 1 tree, 31 leaves, max depth = 11, train loss: 21.78147, val loss: 13.35444, in 0.006s\n",
      "[255/10000] 1 tree, 31 leaves, max depth = 9, train loss: 21.76104, val loss: 13.32710, in 0.010s\n",
      "[256/10000] 1 tree, 31 leaves, max depth = 10, train loss: 21.73707, val loss: 13.30140, in 0.005s\n",
      "[257/10000] 1 tree, 31 leaves, max depth = 8, train loss: 21.72923, val loss: 13.28952, in 0.005s\n",
      "[258/10000] 1 tree, 31 leaves, max depth = 10, train loss: 21.70163, val loss: 13.23170, in 0.005s\n",
      "[259/10000] 1 tree, 31 leaves, max depth = 9, train loss: 21.68115, val loss: 13.20545, in 0.005s\n",
      "[260/10000] 1 tree, 31 leaves, max depth = 11, train loss: 21.65866, val loss: 13.17367, in 0.005s\n",
      "[261/10000] 1 tree, 31 leaves, max depth = 10, train loss: 21.63265, val loss: 13.13105, in 0.006s\n",
      "[262/10000] 1 tree, 31 leaves, max depth = 9, train loss: 21.61001, val loss: 13.08957, in 0.006s\n",
      "[263/10000] 1 tree, 31 leaves, max depth = 11, train loss: 21.58987, val loss: 13.05796, in 0.010s\n",
      "[264/10000] 1 tree, 31 leaves, max depth = 11, train loss: 21.56989, val loss: 13.02663, in 0.006s\n",
      "[265/10000] 1 tree, 31 leaves, max depth = 14, train loss: 21.54844, val loss: 12.99083, in 0.006s\n",
      "[266/10000] 1 tree, 31 leaves, max depth = 12, train loss: 21.54178, val loss: 12.98675, in 0.006s\n",
      "[267/10000] 1 tree, 31 leaves, max depth = 10, train loss: 21.52181, val loss: 12.94773, in 0.005s\n",
      "[268/10000] 1 tree, 31 leaves, max depth = 10, train loss: 21.51517, val loss: 12.94099, in 0.006s\n",
      "[269/10000] 1 tree, 31 leaves, max depth = 12, train loss: 21.49960, val loss: 12.93773, in 0.010s\n",
      "[270/10000] 1 tree, 31 leaves, max depth = 13, train loss: 21.48415, val loss: 12.93427, in 0.006s\n",
      "[271/10000] 1 tree, 31 leaves, max depth = 10, train loss: 21.47742, val loss: 12.93082, in 0.005s\n",
      "[272/10000] 1 tree, 31 leaves, max depth = 13, train loss: 21.46228, val loss: 12.92677, in 0.005s\n",
      "[273/10000] 1 tree, 31 leaves, max depth = 14, train loss: 21.45031, val loss: 12.90242, in 0.007s\n",
      "[274/10000] 1 tree, 31 leaves, max depth = 12, train loss: 21.41258, val loss: 12.87581, in 0.005s\n",
      "[275/10000] 1 tree, 31 leaves, max depth = 12, train loss: 21.39402, val loss: 12.85786, in 0.005s\n",
      "[276/10000] 1 tree, 31 leaves, max depth = 11, train loss: 21.37565, val loss: 12.84017, in 0.009s\n",
      "[277/10000] 1 tree, 31 leaves, max depth = 11, train loss: 21.35749, val loss: 12.82287, in 0.006s\n",
      "[278/10000] 1 tree, 31 leaves, max depth = 9, train loss: 21.35176, val loss: 12.81891, in 0.006s\n",
      "[279/10000] 1 tree, 31 leaves, max depth = 12, train loss: 21.32919, val loss: 12.78579, in 0.006s\n",
      "[280/10000] 1 tree, 31 leaves, max depth = 12, train loss: 21.31433, val loss: 12.78235, in 0.007s\n",
      "[281/10000] 1 tree, 31 leaves, max depth = 11, train loss: 21.30794, val loss: 12.78120, in 0.006s\n",
      "[282/10000] 1 tree, 31 leaves, max depth = 9, train loss: 21.30251, val loss: 12.77750, in 0.019s\n",
      "[283/10000] 1 tree, 31 leaves, max depth = 12, train loss: 21.29284, val loss: 12.76787, in 0.006s\n",
      "[284/10000] 1 tree, 31 leaves, max depth = 11, train loss: 21.28278, val loss: 12.75876, in 0.006s\n",
      "[285/10000] 1 tree, 31 leaves, max depth = 12, train loss: 21.27301, val loss: 12.74981, in 0.007s\n",
      "[286/10000] 1 tree, 31 leaves, max depth = 13, train loss: 21.25552, val loss: 12.71367, in 0.006s\n",
      "[287/10000] 1 tree, 31 leaves, max depth = 11, train loss: 21.23753, val loss: 12.66820, in 0.006s\n",
      "[288/10000] 1 tree, 31 leaves, max depth = 9, train loss: 21.21941, val loss: 12.63683, in 0.011s\n",
      "[289/10000] 1 tree, 31 leaves, max depth = 11, train loss: 21.19627, val loss: 12.58481, in 0.006s\n",
      "[290/10000] 1 tree, 31 leaves, max depth = 11, train loss: 21.18693, val loss: 12.57621, in 0.006s\n",
      "[291/10000] 1 tree, 31 leaves, max depth = 12, train loss: 21.15272, val loss: 12.51577, in 0.005s\n",
      "[292/10000] 1 tree, 31 leaves, max depth = 11, train loss: 21.11932, val loss: 12.49593, in 0.006s\n",
      "[293/10000] 1 tree, 31 leaves, max depth = 10, train loss: 21.08645, val loss: 12.55318, in 0.006s\n",
      "[294/10000] 1 tree, 31 leaves, max depth = 10, train loss: 21.06377, val loss: 12.59789, in 0.006s\n",
      "[295/10000] 1 tree, 31 leaves, max depth = 8, train loss: 21.05869, val loss: 12.59644, in 0.005s\n",
      "[296/10000] 1 tree, 31 leaves, max depth = 10, train loss: 21.05367, val loss: 12.59588, in 0.006s\n",
      "[297/10000] 1 tree, 31 leaves, max depth = 11, train loss: 21.03216, val loss: 12.64032, in 0.010s\n",
      "[298/10000] 1 tree, 31 leaves, max depth = 10, train loss: 21.02717, val loss: 12.64009, in 0.006s\n",
      "[299/10000] 1 tree, 31 leaves, max depth = 10, train loss: 21.02238, val loss: 12.63973, in 0.005s\n",
      "[300/10000] 1 tree, 31 leaves, max depth = 12, train loss: 21.00901, val loss: 12.65941, in 0.007s\n",
      "[301/10000] 1 tree, 31 leaves, max depth = 12, train loss: 20.99675, val loss: 12.68061, in 0.006s\n",
      "[302/10000] 1 tree, 31 leaves, max depth = 12, train loss: 20.98627, val loss: 12.70601, in 0.006s\n",
      "Fit 302 trees in 2.380 s, (9362 total leaves)\n",
      "Time spent computing histograms: 0.471s\n",
      "Time spent finding best splits:  0.326s\n",
      "Time spent applying splits:      0.332s\n",
      "Time spent predicting:           0.079s\n"
     ]
    },
    {
     "data": {
      "text/plain": "HistGradientBoostingRegressor(l2_regularization=0.02, learning_rate=0.01,\n                              loss='absolute_error', max_bins=156,\n                              max_iter=10000, min_samples_leaf=8,\n                              random_state=46, validation_fraction=0.01,\n                              verbose=999)",
      "text/html": "<style>#sk-container-id-55 {color: black;background-color: white;}#sk-container-id-55 pre{padding: 0;}#sk-container-id-55 div.sk-toggleable {background-color: white;}#sk-container-id-55 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-55 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-55 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-55 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-55 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-55 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-55 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-55 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-55 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-55 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-55 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-55 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-55 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-55 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-55 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-55 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-55 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-55 div.sk-item {position: relative;z-index: 1;}#sk-container-id-55 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-55 div.sk-item::before, #sk-container-id-55 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-55 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-55 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-55 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-55 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-55 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-55 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-55 div.sk-label-container {text-align: center;}#sk-container-id-55 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-55 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-55\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HistGradientBoostingRegressor(l2_regularization=0.02, learning_rate=0.01,\n                              loss=&#x27;absolute_error&#x27;, max_bins=156,\n                              max_iter=10000, min_samples_leaf=8,\n                              random_state=46, validation_fraction=0.01,\n                              verbose=999)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-55\" type=\"checkbox\" checked><label for=\"sk-estimator-id-55\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HistGradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>HistGradientBoostingRegressor(l2_regularization=0.02, learning_rate=0.01,\n                              loss=&#x27;absolute_error&#x27;, max_bins=156,\n                              max_iter=10000, min_samples_leaf=8,\n                              random_state=46, validation_fraction=0.01,\n                              verbose=999)</pre></div></div></div></div></div>"
     },
     "execution_count": 1030,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boost_model = HistGradientBoostingRegressor(learning_rate=0.01, verbose=999, max_depth=None, min_samples_leaf=8,\n",
    "                                            max_iter=10000, loss='absolute_error', l2_regularization=0.02, max_bins=156, # 245, 200\n",
    "                                            validation_fraction=0.01, random_state=46)\n",
    "\n",
    "boost_model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1031,
   "outputs": [],
   "source": [
    "# SelectKBest(mutual_info_regression, k=9).fit_transform(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1032,
   "outputs": [],
   "source": [
    "def score(predicted: np.ndarray, expected: np.ndarray):\n",
    "    def smape(A, F):\n",
    "        return 100 / len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))\n",
    "\n",
    "    print(f'Smape: {smape(expected, predicted)}')\n",
    "    print(f'R2: {r2_score(expected, predicted)}')\n",
    "    print(f'RMSE: {mean_squared_error(expected, predicted)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1033,
   "outputs": [],
   "source": [
    "def postprocess(output: np.ndarray) -> np.ndarray:\n",
    "    squarer = lambda t: round(t)\n",
    "    vfunc = np.vectorize(squarer)\n",
    "    return vfunc(output)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1034,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[126   0   0 ...  27   0   0]\n",
      "4375      85.0\n",
      "6682       0.0\n",
      "23737      0.0\n",
      "5324     204.0\n",
      "17068    106.0\n",
      "         ...  \n",
      "24667      0.0\n",
      "24589      0.0\n",
      "20333     19.0\n",
      "14736      0.0\n",
      "16083      0.0\n",
      "Name: Sales, Length: 8271, dtype: float64\n",
      "Smape: 19.427030858053463\n",
      "R2: 0.3077044880542944\n",
      "RMSE: 33992.37794704389\n"
     ]
    }
   ],
   "source": [
    "predicted = postprocess(boost_model.predict(X_test))\n",
    "print(predicted)\n",
    "print(y_test)\n",
    "score(predicted, y_test)\n",
    "# 20.727624784060993"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1035,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binning 0.003 GB of training data: 0.055 s\n",
      "Binning 0.000 GB of validation data: 0.000 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/10000] 1 tree, 31 leaves, max depth = 9, train loss: 38.57388, val loss: 62.39231, in 0.007s\n",
      "[2/10000] 1 tree, 31 leaves, max depth = 8, train loss: 38.48668, val loss: 62.30992, in 0.096s\n",
      "[3/10000] 1 tree, 31 leaves, max depth = 8, train loss: 38.40035, val loss: 62.22836, in 0.100s\n",
      "[4/10000] 1 tree, 31 leaves, max depth = 8, train loss: 38.31489, val loss: 62.14762, in 0.010s\n",
      "[5/10000] 1 tree, 31 leaves, max depth = 8, train loss: 38.23028, val loss: 62.06768, in 0.007s\n",
      "[6/10000] 1 tree, 31 leaves, max depth = 8, train loss: 38.14693, val loss: 61.98854, in 0.054s\n",
      "[7/10000] 1 tree, 31 leaves, max depth = 9, train loss: 38.05689, val loss: 61.89635, in 0.009s\n",
      "[8/10000] 1 tree, 31 leaves, max depth = 8, train loss: 37.96769, val loss: 61.80508, in 0.007s\n",
      "[9/10000] 1 tree, 31 leaves, max depth = 8, train loss: 37.87936, val loss: 61.71472, in 0.012s\n",
      "[10/10000] 1 tree, 31 leaves, max depth = 8, train loss: 37.79191, val loss: 61.62526, in 0.009s\n",
      "[11/10000] 1 tree, 31 leaves, max depth = 8, train loss: 37.70606, val loss: 61.53670, in 0.006s\n",
      "[12/10000] 1 tree, 31 leaves, max depth = 9, train loss: 37.61444, val loss: 61.43518, in 0.015s\n",
      "[13/10000] 1 tree, 31 leaves, max depth = 9, train loss: 37.52369, val loss: 61.33468, in 0.006s\n",
      "[14/10000] 1 tree, 31 leaves, max depth = 9, train loss: 37.43389, val loss: 61.23518, in 0.006s\n",
      "[15/10000] 1 tree, 31 leaves, max depth = 9, train loss: 37.34495, val loss: 61.13667, in 0.005s\n",
      "[16/10000] 1 tree, 31 leaves, max depth = 9, train loss: 37.25905, val loss: 61.03915, in 0.005s\n",
      "[17/10000] 1 tree, 31 leaves, max depth = 8, train loss: 37.16405, val loss: 60.92530, in 0.009s\n",
      "[18/10000] 1 tree, 31 leaves, max depth = 8, train loss: 37.06923, val loss: 60.81129, in 0.006s\n",
      "[19/10000] 1 tree, 31 leaves, max depth = 7, train loss: 36.97692, val loss: 60.69971, in 0.007s\n",
      "[20/10000] 1 tree, 31 leaves, max depth = 8, train loss: 36.87862, val loss: 60.57194, in 0.009s\n",
      "[21/10000] 1 tree, 31 leaves, max depth = 8, train loss: 36.78062, val loss: 60.44419, in 0.005s\n",
      "[22/10000] 1 tree, 31 leaves, max depth = 7, train loss: 36.68484, val loss: 60.31897, in 0.006s\n",
      "[23/10000] 1 tree, 31 leaves, max depth = 7, train loss: 36.59096, val loss: 60.21827, in 0.006s\n",
      "[24/10000] 1 tree, 31 leaves, max depth = 7, train loss: 36.49502, val loss: 60.10185, in 0.005s\n",
      "[25/10000] 1 tree, 31 leaves, max depth = 7, train loss: 36.40006, val loss: 59.98660, in 0.006s\n",
      "[26/10000] 1 tree, 31 leaves, max depth = 7, train loss: 36.30705, val loss: 59.87251, in 0.006s\n",
      "[27/10000] 1 tree, 31 leaves, max depth = 7, train loss: 36.21172, val loss: 59.75039, in 0.008s\n",
      "[28/10000] 1 tree, 31 leaves, max depth = 7, train loss: 36.11736, val loss: 59.62949, in 0.006s\n",
      "[29/10000] 1 tree, 31 leaves, max depth = 7, train loss: 36.02541, val loss: 59.50980, in 0.006s\n",
      "[30/10000] 1 tree, 31 leaves, max depth = 8, train loss: 35.92659, val loss: 59.38744, in 0.006s\n",
      "[31/10000] 1 tree, 31 leaves, max depth = 8, train loss: 35.82834, val loss: 59.26629, in 0.006s\n",
      "[32/10000] 1 tree, 31 leaves, max depth = 8, train loss: 35.72697, val loss: 59.13290, in 0.006s\n",
      "[33/10000] 1 tree, 31 leaves, max depth = 8, train loss: 35.62666, val loss: 59.00084, in 0.006s\n",
      "[34/10000] 1 tree, 31 leaves, max depth = 8, train loss: 35.52843, val loss: 58.87010, in 0.008s\n",
      "[35/10000] 1 tree, 31 leaves, max depth = 8, train loss: 35.43076, val loss: 58.74067, in 0.006s\n",
      "[36/10000] 1 tree, 31 leaves, max depth = 9, train loss: 35.33405, val loss: 58.61253, in 0.006s\n",
      "[37/10000] 1 tree, 31 leaves, max depth = 8, train loss: 35.23084, val loss: 58.46414, in 0.006s\n",
      "[38/10000] 1 tree, 31 leaves, max depth = 8, train loss: 35.12876, val loss: 58.31723, in 0.006s\n",
      "[39/10000] 1 tree, 31 leaves, max depth = 8, train loss: 35.02957, val loss: 58.17448, in 0.007s\n",
      "[40/10000] 1 tree, 31 leaves, max depth = 8, train loss: 34.92543, val loss: 58.01162, in 0.006s\n",
      "[41/10000] 1 tree, 31 leaves, max depth = 8, train loss: 34.82305, val loss: 57.85923, in 0.006s\n",
      "[42/10000] 1 tree, 31 leaves, max depth = 8, train loss: 34.72318, val loss: 57.71643, in 0.016s\n",
      "[43/10000] 1 tree, 31 leaves, max depth = 8, train loss: 34.62889, val loss: 57.58044, in 0.007s\n",
      "[44/10000] 1 tree, 31 leaves, max depth = 8, train loss: 34.52527, val loss: 57.41890, in 0.006s\n",
      "[45/10000] 1 tree, 31 leaves, max depth = 8, train loss: 34.42186, val loss: 57.24281, in 0.007s\n",
      "[46/10000] 1 tree, 31 leaves, max depth = 8, train loss: 34.31983, val loss: 57.06849, in 0.006s\n",
      "[47/10000] 1 tree, 31 leaves, max depth = 8, train loss: 34.21628, val loss: 56.89860, in 0.012s\n",
      "[48/10000] 1 tree, 31 leaves, max depth = 7, train loss: 34.11183, val loss: 56.71960, in 0.006s\n",
      "[49/10000] 1 tree, 31 leaves, max depth = 8, train loss: 34.00889, val loss: 56.54240, in 0.006s\n",
      "[50/10000] 1 tree, 31 leaves, max depth = 8, train loss: 33.90262, val loss: 56.34816, in 0.006s\n",
      "[51/10000] 1 tree, 31 leaves, max depth = 8, train loss: 33.79812, val loss: 56.15585, in 0.006s\n",
      "[52/10000] 1 tree, 31 leaves, max depth = 8, train loss: 33.69995, val loss: 55.97894, in 0.006s\n",
      "[53/10000] 1 tree, 31 leaves, max depth = 8, train loss: 33.59460, val loss: 55.77148, in 0.006s\n",
      "[54/10000] 1 tree, 31 leaves, max depth = 8, train loss: 33.49941, val loss: 55.59764, in 0.006s\n",
      "[55/10000] 1 tree, 31 leaves, max depth = 8, train loss: 33.39113, val loss: 55.37014, in 0.009s\n",
      "[56/10000] 1 tree, 31 leaves, max depth = 8, train loss: 33.28282, val loss: 55.15262, in 0.006s\n",
      "[57/10000] 1 tree, 31 leaves, max depth = 8, train loss: 33.16600, val loss: 54.92133, in 0.007s\n",
      "[58/10000] 1 tree, 31 leaves, max depth = 8, train loss: 33.06729, val loss: 54.73321, in 0.007s\n",
      "[59/10000] 1 tree, 31 leaves, max depth = 8, train loss: 32.96149, val loss: 54.51467, in 0.006s\n",
      "[60/10000] 1 tree, 31 leaves, max depth = 8, train loss: 32.84765, val loss: 54.29306, in 0.006s\n",
      "[61/10000] 1 tree, 31 leaves, max depth = 8, train loss: 32.73446, val loss: 54.10311, in 0.008s\n",
      "[62/10000] 1 tree, 31 leaves, max depth = 8, train loss: 32.62411, val loss: 53.88205, in 0.006s\n",
      "[63/10000] 1 tree, 31 leaves, max depth = 8, train loss: 32.52065, val loss: 53.67632, in 0.006s\n",
      "[64/10000] 1 tree, 31 leaves, max depth = 7, train loss: 32.41630, val loss: 53.46411, in 0.007s\n",
      "[65/10000] 1 tree, 31 leaves, max depth = 8, train loss: 32.30243, val loss: 53.25051, in 0.006s\n",
      "[66/10000] 1 tree, 31 leaves, max depth = 7, train loss: 32.19516, val loss: 53.04279, in 0.006s\n",
      "[67/10000] 1 tree, 31 leaves, max depth = 10, train loss: 32.09238, val loss: 52.86419, in 0.006s\n",
      "[68/10000] 1 tree, 31 leaves, max depth = 7, train loss: 31.97928, val loss: 52.66641, in 0.006s\n",
      "[69/10000] 1 tree, 31 leaves, max depth = 8, train loss: 31.86492, val loss: 52.45947, in 0.009s\n",
      "[70/10000] 1 tree, 31 leaves, max depth = 8, train loss: 31.74869, val loss: 52.21273, in 0.006s\n",
      "[71/10000] 1 tree, 31 leaves, max depth = 8, train loss: 31.63172, val loss: 51.98249, in 0.006s\n",
      "[72/10000] 1 tree, 31 leaves, max depth = 9, train loss: 31.52166, val loss: 51.78966, in 0.006s\n",
      "[73/10000] 1 tree, 31 leaves, max depth = 8, train loss: 31.40653, val loss: 51.56357, in 0.006s\n",
      "[74/10000] 1 tree, 31 leaves, max depth = 8, train loss: 31.29672, val loss: 51.36374, in 0.006s\n",
      "[75/10000] 1 tree, 31 leaves, max depth = 7, train loss: 31.18421, val loss: 51.14839, in 0.006s\n",
      "[76/10000] 1 tree, 31 leaves, max depth = 8, train loss: 31.07021, val loss: 50.91248, in 0.008s\n",
      "[77/10000] 1 tree, 31 leaves, max depth = 8, train loss: 30.95801, val loss: 50.67893, in 0.006s\n",
      "[78/10000] 1 tree, 31 leaves, max depth = 8, train loss: 30.84156, val loss: 50.47330, in 0.006s\n",
      "[79/10000] 1 tree, 31 leaves, max depth = 9, train loss: 30.72783, val loss: 50.27166, in 0.006s\n",
      "[80/10000] 1 tree, 31 leaves, max depth = 9, train loss: 30.61448, val loss: 50.12095, in 0.006s\n",
      "[81/10000] 1 tree, 31 leaves, max depth = 9, train loss: 30.50318, val loss: 49.96220, in 0.007s\n",
      "[82/10000] 1 tree, 31 leaves, max depth = 8, train loss: 30.39201, val loss: 49.80051, in 0.006s\n",
      "[83/10000] 1 tree, 31 leaves, max depth = 8, train loss: 30.27558, val loss: 49.61296, in 0.006s\n",
      "[84/10000] 1 tree, 31 leaves, max depth = 8, train loss: 30.18597, val loss: 49.46340, in 0.008s\n",
      "[85/10000] 1 tree, 31 leaves, max depth = 8, train loss: 30.08999, val loss: 49.22037, in 0.006s\n",
      "[86/10000] 1 tree, 31 leaves, max depth = 10, train loss: 30.00276, val loss: 49.08209, in 0.006s\n",
      "[87/10000] 1 tree, 31 leaves, max depth = 10, train loss: 29.90198, val loss: 48.89676, in 0.006s\n",
      "[88/10000] 1 tree, 31 leaves, max depth = 7, train loss: 29.79403, val loss: 48.72900, in 0.009s\n",
      "[89/10000] 1 tree, 31 leaves, max depth = 9, train loss: 29.69159, val loss: 48.56937, in 0.007s\n",
      "[90/10000] 1 tree, 31 leaves, max depth = 9, train loss: 29.58593, val loss: 48.41134, in 0.008s\n",
      "[91/10000] 1 tree, 31 leaves, max depth = 10, train loss: 29.47094, val loss: 48.28602, in 0.006s\n",
      "[92/10000] 1 tree, 31 leaves, max depth = 8, train loss: 29.35569, val loss: 48.16023, in 0.006s\n",
      "[93/10000] 1 tree, 31 leaves, max depth = 9, train loss: 29.24567, val loss: 47.97208, in 0.006s\n",
      "[94/10000] 1 tree, 31 leaves, max depth = 9, train loss: 29.14865, val loss: 47.73264, in 0.006s\n",
      "[95/10000] 1 tree, 31 leaves, max depth = 9, train loss: 29.05032, val loss: 47.50157, in 0.006s\n",
      "[96/10000] 1 tree, 31 leaves, max depth = 9, train loss: 28.94341, val loss: 47.31872, in 0.006s\n",
      "[97/10000] 1 tree, 31 leaves, max depth = 8, train loss: 28.84380, val loss: 47.18107, in 0.006s\n",
      "[98/10000] 1 tree, 31 leaves, max depth = 9, train loss: 28.74661, val loss: 47.02932, in 0.006s\n",
      "[99/10000] 1 tree, 31 leaves, max depth = 7, train loss: 28.64120, val loss: 46.91109, in 0.006s\n",
      "[100/10000] 1 tree, 31 leaves, max depth = 8, train loss: 28.54606, val loss: 46.80642, in 0.009s\n",
      "[101/10000] 1 tree, 31 leaves, max depth = 7, train loss: 28.44563, val loss: 46.68967, in 0.006s\n",
      "[102/10000] 1 tree, 31 leaves, max depth = 9, train loss: 28.34122, val loss: 46.53932, in 0.006s\n",
      "[103/10000] 1 tree, 31 leaves, max depth = 9, train loss: 28.22750, val loss: 46.35062, in 0.008s\n",
      "[104/10000] 1 tree, 31 leaves, max depth = 8, train loss: 28.14203, val loss: 46.13579, in 0.006s\n",
      "[105/10000] 1 tree, 31 leaves, max depth = 8, train loss: 28.04098, val loss: 46.00307, in 0.006s\n",
      "[106/10000] 1 tree, 31 leaves, max depth = 8, train loss: 27.93721, val loss: 45.86464, in 0.006s\n",
      "[107/10000] 1 tree, 31 leaves, max depth = 9, train loss: 27.83408, val loss: 45.72697, in 0.008s\n",
      "[108/10000] 1 tree, 31 leaves, max depth = 10, train loss: 27.73268, val loss: 45.59141, in 0.007s\n",
      "[109/10000] 1 tree, 31 leaves, max depth = 9, train loss: 27.63596, val loss: 45.45541, in 0.006s\n",
      "[110/10000] 1 tree, 31 leaves, max depth = 8, train loss: 27.53834, val loss: 45.32031, in 0.006s\n",
      "[111/10000] 1 tree, 31 leaves, max depth = 8, train loss: 27.44780, val loss: 45.17794, in 0.008s\n",
      "[112/10000] 1 tree, 31 leaves, max depth = 9, train loss: 27.34777, val loss: 45.02820, in 0.006s\n",
      "[113/10000] 1 tree, 31 leaves, max depth = 8, train loss: 27.25829, val loss: 44.89370, in 0.006s\n",
      "[114/10000] 1 tree, 31 leaves, max depth = 11, train loss: 27.16745, val loss: 44.76634, in 0.006s\n",
      "[115/10000] 1 tree, 31 leaves, max depth = 8, train loss: 27.07474, val loss: 44.61166, in 0.009s\n",
      "[116/10000] 1 tree, 31 leaves, max depth = 9, train loss: 26.98843, val loss: 44.40278, in 0.006s\n",
      "[117/10000] 1 tree, 31 leaves, max depth = 8, train loss: 26.90126, val loss: 44.28090, in 0.015s\n",
      "[118/10000] 1 tree, 31 leaves, max depth = 8, train loss: 26.82099, val loss: 44.11903, in 0.006s\n",
      "[119/10000] 1 tree, 31 leaves, max depth = 9, train loss: 26.73093, val loss: 43.95399, in 0.006s\n",
      "[120/10000] 1 tree, 31 leaves, max depth = 10, train loss: 26.64466, val loss: 43.81476, in 0.009s\n",
      "[121/10000] 1 tree, 31 leaves, max depth = 10, train loss: 26.55708, val loss: 43.68006, in 0.006s\n",
      "[122/10000] 1 tree, 31 leaves, max depth = 8, train loss: 26.45845, val loss: 43.47058, in 0.007s\n",
      "[123/10000] 1 tree, 31 leaves, max depth = 8, train loss: 26.36510, val loss: 43.31291, in 0.006s\n",
      "[124/10000] 1 tree, 31 leaves, max depth = 11, train loss: 26.28280, val loss: 43.23687, in 0.006s\n",
      "[125/10000] 1 tree, 31 leaves, max depth = 8, train loss: 26.19453, val loss: 43.08749, in 0.006s\n",
      "[126/10000] 1 tree, 31 leaves, max depth = 8, train loss: 26.12602, val loss: 42.93096, in 0.006s\n",
      "[127/10000] 1 tree, 31 leaves, max depth = 8, train loss: 26.03116, val loss: 42.72643, in 0.009s\n",
      "[128/10000] 1 tree, 31 leaves, max depth = 8, train loss: 25.96792, val loss: 42.56581, in 0.006s\n",
      "[129/10000] 1 tree, 31 leaves, max depth = 9, train loss: 25.88399, val loss: 42.41063, in 0.006s\n",
      "[130/10000] 1 tree, 31 leaves, max depth = 10, train loss: 25.80315, val loss: 42.25825, in 0.006s\n",
      "[131/10000] 1 tree, 31 leaves, max depth = 8, train loss: 25.71653, val loss: 42.06778, in 0.007s\n",
      "[132/10000] 1 tree, 31 leaves, max depth = 9, train loss: 25.63706, val loss: 41.93054, in 0.006s\n",
      "[133/10000] 1 tree, 31 leaves, max depth = 8, train loss: 25.55924, val loss: 41.79679, in 0.006s\n",
      "[134/10000] 1 tree, 31 leaves, max depth = 8, train loss: 25.47506, val loss: 41.62371, in 0.009s\n",
      "[135/10000] 1 tree, 31 leaves, max depth = 8, train loss: 25.39194, val loss: 41.45245, in 0.006s\n",
      "[136/10000] 1 tree, 31 leaves, max depth = 8, train loss: 25.29917, val loss: 41.22221, in 0.006s\n",
      "[137/10000] 1 tree, 31 leaves, max depth = 10, train loss: 25.22860, val loss: 41.15854, in 0.006s\n",
      "[138/10000] 1 tree, 31 leaves, max depth = 9, train loss: 25.16202, val loss: 41.09925, in 0.006s\n",
      "[139/10000] 1 tree, 31 leaves, max depth = 11, train loss: 25.09399, val loss: 41.03858, in 0.006s\n",
      "[140/10000] 1 tree, 31 leaves, max depth = 8, train loss: 25.04145, val loss: 40.99225, in 0.006s\n",
      "[141/10000] 1 tree, 31 leaves, max depth = 8, train loss: 24.96697, val loss: 40.81899, in 0.009s\n",
      "[142/10000] 1 tree, 31 leaves, max depth = 9, train loss: 24.91598, val loss: 40.71261, in 0.006s\n",
      "[143/10000] 1 tree, 31 leaves, max depth = 8, train loss: 24.83754, val loss: 40.51331, in 0.006s\n",
      "[144/10000] 1 tree, 31 leaves, max depth = 8, train loss: 24.76996, val loss: 40.45134, in 0.009s\n",
      "[145/10000] 1 tree, 31 leaves, max depth = 9, train loss: 24.69711, val loss: 40.28862, in 0.006s\n",
      "[146/10000] 1 tree, 31 leaves, max depth = 8, train loss: 24.62543, val loss: 40.12501, in 0.017s\n",
      "[147/10000] 1 tree, 31 leaves, max depth = 8, train loss: 24.56130, val loss: 39.99061, in 0.065s\n",
      "[148/10000] 1 tree, 31 leaves, max depth = 9, train loss: 24.48792, val loss: 39.76977, in 0.023s\n",
      "[149/10000] 1 tree, 31 leaves, max depth = 9, train loss: 24.41844, val loss: 39.61554, in 0.006s\n",
      "[150/10000] 1 tree, 31 leaves, max depth = 8, train loss: 24.36157, val loss: 39.50805, in 0.006s\n",
      "[151/10000] 1 tree, 31 leaves, max depth = 8, train loss: 24.29971, val loss: 39.38169, in 0.006s\n",
      "[152/10000] 1 tree, 31 leaves, max depth = 8, train loss: 24.23491, val loss: 39.21617, in 0.008s\n",
      "[153/10000] 1 tree, 31 leaves, max depth = 9, train loss: 24.17841, val loss: 39.15217, in 0.008s\n",
      "[154/10000] 1 tree, 31 leaves, max depth = 8, train loss: 24.12413, val loss: 39.08881, in 0.015s\n",
      "[155/10000] 1 tree, 31 leaves, max depth = 8, train loss: 24.04927, val loss: 38.87498, in 0.008s\n",
      "[156/10000] 1 tree, 31 leaves, max depth = 8, train loss: 23.98820, val loss: 38.69622, in 0.009s\n",
      "[157/10000] 1 tree, 31 leaves, max depth = 8, train loss: 23.91947, val loss: 38.49988, in 0.009s\n",
      "[158/10000] 1 tree, 31 leaves, max depth = 10, train loss: 23.86910, val loss: 38.38465, in 0.011s\n",
      "[159/10000] 1 tree, 31 leaves, max depth = 8, train loss: 23.83122, val loss: 38.33289, in 0.011s\n",
      "[160/10000] 1 tree, 31 leaves, max depth = 9, train loss: 23.76524, val loss: 38.14304, in 0.009s\n",
      "[161/10000] 1 tree, 31 leaves, max depth = 9, train loss: 23.70032, val loss: 37.93919, in 0.010s\n",
      "[162/10000] 1 tree, 31 leaves, max depth = 9, train loss: 23.63496, val loss: 37.75694, in 0.009s\n",
      "[163/10000] 1 tree, 31 leaves, max depth = 8, train loss: 23.57612, val loss: 37.61351, in 0.015s\n",
      "[164/10000] 1 tree, 31 leaves, max depth = 10, train loss: 23.52163, val loss: 37.42080, in 0.008s\n",
      "[165/10000] 1 tree, 31 leaves, max depth = 8, train loss: 23.45661, val loss: 37.22902, in 0.010s\n",
      "[166/10000] 1 tree, 31 leaves, max depth = 9, train loss: 23.41170, val loss: 37.14251, in 0.009s\n",
      "[167/10000] 1 tree, 31 leaves, max depth = 9, train loss: 23.35936, val loss: 37.00942, in 0.014s\n",
      "[168/10000] 1 tree, 31 leaves, max depth = 8, train loss: 23.31387, val loss: 36.85749, in 0.009s\n",
      "[169/10000] 1 tree, 31 leaves, max depth = 11, train loss: 23.26791, val loss: 36.70348, in 0.021s\n",
      "[170/10000] 1 tree, 31 leaves, max depth = 11, train loss: 23.21882, val loss: 36.48428, in 0.009s\n",
      "[171/10000] 1 tree, 31 leaves, max depth = 10, train loss: 23.16237, val loss: 36.27949, in 0.008s\n",
      "[172/10000] 1 tree, 31 leaves, max depth = 9, train loss: 23.12441, val loss: 36.20560, in 0.015s\n",
      "[173/10000] 1 tree, 31 leaves, max depth = 8, train loss: 23.06925, val loss: 36.00266, in 0.008s\n",
      "[174/10000] 1 tree, 31 leaves, max depth = 8, train loss: 23.02975, val loss: 35.95283, in 0.009s\n",
      "[175/10000] 1 tree, 31 leaves, max depth = 10, train loss: 22.97619, val loss: 35.75601, in 0.006s\n",
      "[176/10000] 1 tree, 31 leaves, max depth = 9, train loss: 22.93532, val loss: 35.72014, in 0.007s\n",
      "[177/10000] 1 tree, 31 leaves, max depth = 9, train loss: 22.89975, val loss: 35.56714, in 0.014s\n",
      "[178/10000] 1 tree, 31 leaves, max depth = 8, train loss: 22.85072, val loss: 35.52167, in 0.006s\n",
      "[179/10000] 1 tree, 31 leaves, max depth = 9, train loss: 22.80441, val loss: 35.48681, in 0.006s\n",
      "[180/10000] 1 tree, 31 leaves, max depth = 9, train loss: 22.77149, val loss: 35.42436, in 0.007s\n",
      "[181/10000] 1 tree, 31 leaves, max depth = 9, train loss: 22.73923, val loss: 35.36254, in 0.006s\n",
      "[182/10000] 1 tree, 31 leaves, max depth = 9, train loss: 22.70138, val loss: 35.21012, in 0.009s\n",
      "[183/10000] 1 tree, 31 leaves, max depth = 10, train loss: 22.65687, val loss: 35.18322, in 0.006s\n",
      "[184/10000] 1 tree, 31 leaves, max depth = 8, train loss: 22.62397, val loss: 35.08893, in 0.006s\n",
      "[185/10000] 1 tree, 31 leaves, max depth = 8, train loss: 22.58245, val loss: 34.90608, in 0.007s\n",
      "[186/10000] 1 tree, 31 leaves, max depth = 8, train loss: 22.55058, val loss: 34.81294, in 0.006s\n",
      "[187/10000] 1 tree, 31 leaves, max depth = 10, train loss: 22.51517, val loss: 34.62161, in 0.007s\n",
      "[188/10000] 1 tree, 31 leaves, max depth = 11, train loss: 22.49139, val loss: 34.57346, in 0.006s\n",
      "[189/10000] 1 tree, 31 leaves, max depth = 12, train loss: 22.46161, val loss: 34.48123, in 0.006s\n",
      "[190/10000] 1 tree, 31 leaves, max depth = 11, train loss: 22.42681, val loss: 34.36628, in 0.062s\n",
      "[191/10000] 1 tree, 31 leaves, max depth = 8, train loss: 22.39981, val loss: 34.24963, in 0.007s\n",
      "[192/10000] 1 tree, 31 leaves, max depth = 11, train loss: 22.36215, val loss: 34.08534, in 0.007s\n",
      "[193/10000] 1 tree, 31 leaves, max depth = 9, train loss: 22.32220, val loss: 33.90467, in 0.012s\n",
      "[194/10000] 1 tree, 31 leaves, max depth = 9, train loss: 22.28850, val loss: 33.66489, in 0.007s\n",
      "[195/10000] 1 tree, 31 leaves, max depth = 8, train loss: 22.27175, val loss: 33.60651, in 0.007s\n",
      "[196/10000] 1 tree, 31 leaves, max depth = 10, train loss: 22.23091, val loss: 33.37089, in 0.007s\n",
      "[197/10000] 1 tree, 31 leaves, max depth = 8, train loss: 22.20626, val loss: 33.31859, in 0.009s\n",
      "[198/10000] 1 tree, 31 leaves, max depth = 9, train loss: 22.18962, val loss: 33.28023, in 0.008s\n",
      "[199/10000] 1 tree, 31 leaves, max depth = 9, train loss: 22.15713, val loss: 33.04442, in 0.010s\n",
      "[200/10000] 1 tree, 31 leaves, max depth = 9, train loss: 22.12901, val loss: 32.96769, in 0.007s\n",
      "[201/10000] 1 tree, 31 leaves, max depth = 9, train loss: 22.09018, val loss: 32.88768, in 0.006s\n",
      "[202/10000] 1 tree, 31 leaves, max depth = 9, train loss: 22.05679, val loss: 32.77895, in 0.006s\n",
      "[203/10000] 1 tree, 31 leaves, max depth = 9, train loss: 22.02465, val loss: 32.66509, in 0.006s\n",
      "[204/10000] 1 tree, 31 leaves, max depth = 8, train loss: 21.98166, val loss: 32.59458, in 0.006s\n",
      "[205/10000] 1 tree, 31 leaves, max depth = 8, train loss: 21.94617, val loss: 32.40962, in 0.006s\n",
      "[206/10000] 1 tree, 31 leaves, max depth = 9, train loss: 21.91107, val loss: 32.38780, in 0.006s\n",
      "[207/10000] 1 tree, 31 leaves, max depth = 9, train loss: 21.87129, val loss: 32.19241, in 0.006s\n",
      "[208/10000] 1 tree, 31 leaves, max depth = 9, train loss: 21.83242, val loss: 31.99896, in 0.006s\n",
      "[209/10000] 1 tree, 31 leaves, max depth = 10, train loss: 21.80666, val loss: 31.94590, in 0.216s\n",
      "[210/10000] 1 tree, 31 leaves, max depth = 9, train loss: 21.78200, val loss: 31.90522, in 0.007s\n",
      "[211/10000] 1 tree, 31 leaves, max depth = 9, train loss: 21.75059, val loss: 31.78776, in 0.008s\n",
      "[212/10000] 1 tree, 31 leaves, max depth = 8, train loss: 21.73917, val loss: 31.75556, in 0.006s\n",
      "[213/10000] 1 tree, 31 leaves, max depth = 10, train loss: 21.70211, val loss: 31.56516, in 0.009s\n",
      "[214/10000] 1 tree, 31 leaves, max depth = 9, train loss: 21.67115, val loss: 31.39853, in 0.005s\n",
      "[215/10000] 1 tree, 31 leaves, max depth = 10, train loss: 21.65268, val loss: 31.26552, in 0.005s\n",
      "[216/10000] 1 tree, 31 leaves, max depth = 9, train loss: 21.62475, val loss: 31.04307, in 0.006s\n",
      "[217/10000] 1 tree, 31 leaves, max depth = 10, train loss: 21.58972, val loss: 30.84798, in 0.005s\n",
      "[218/10000] 1 tree, 31 leaves, max depth = 9, train loss: 21.55785, val loss: 30.84000, in 0.006s\n",
      "[219/10000] 1 tree, 31 leaves, max depth = 9, train loss: 21.52680, val loss: 30.83666, in 0.007s\n",
      "[220/10000] 1 tree, 31 leaves, max depth = 8, train loss: 21.51547, val loss: 30.80580, in 0.006s\n",
      "[221/10000] 1 tree, 31 leaves, max depth = 9, train loss: 21.48879, val loss: 30.70438, in 0.006s\n",
      "[222/10000] 1 tree, 31 leaves, max depth = 9, train loss: 21.45872, val loss: 30.60466, in 0.009s\n",
      "[223/10000] 1 tree, 31 leaves, max depth = 9, train loss: 21.42296, val loss: 30.49448, in 0.006s\n",
      "[224/10000] 1 tree, 31 leaves, max depth = 10, train loss: 21.40364, val loss: 30.35620, in 0.006s\n",
      "[225/10000] 1 tree, 31 leaves, max depth = 8, train loss: 21.37486, val loss: 30.26647, in 0.006s\n",
      "[226/10000] 1 tree, 31 leaves, max depth = 10, train loss: 21.35605, val loss: 30.13290, in 0.007s\n",
      "[227/10000] 1 tree, 31 leaves, max depth = 10, train loss: 21.33626, val loss: 30.01800, in 0.007s\n",
      "[228/10000] 1 tree, 31 leaves, max depth = 9, train loss: 21.30197, val loss: 29.91301, in 0.006s\n",
      "[229/10000] 1 tree, 31 leaves, max depth = 9, train loss: 21.28421, val loss: 29.72704, in 0.010s\n",
      "[230/10000] 1 tree, 31 leaves, max depth = 8, train loss: 21.26678, val loss: 29.67912, in 0.006s\n",
      "[231/10000] 1 tree, 31 leaves, max depth = 9, train loss: 21.24756, val loss: 29.56841, in 0.006s\n",
      "[232/10000] 1 tree, 31 leaves, max depth = 8, train loss: 21.21698, val loss: 29.39296, in 0.006s\n",
      "[233/10000] 1 tree, 31 leaves, max depth = 10, train loss: 21.19259, val loss: 29.29226, in 0.050s\n",
      "[234/10000] 1 tree, 31 leaves, max depth = 10, train loss: 21.15914, val loss: 29.19231, in 0.015s\n",
      "[235/10000] 1 tree, 31 leaves, max depth = 10, train loss: 21.14180, val loss: 29.06235, in 0.012s\n",
      "[236/10000] 1 tree, 31 leaves, max depth = 10, train loss: 21.11569, val loss: 28.97239, in 0.015s\n",
      "[237/10000] 1 tree, 31 leaves, max depth = 10, train loss: 21.09894, val loss: 28.84544, in 0.010s\n",
      "[238/10000] 1 tree, 31 leaves, max depth = 10, train loss: 21.08216, val loss: 28.74276, in 0.012s\n",
      "[239/10000] 1 tree, 31 leaves, max depth = 8, train loss: 21.05630, val loss: 28.64488, in 0.008s\n",
      "[240/10000] 1 tree, 31 leaves, max depth = 8, train loss: 21.02856, val loss: 28.49118, in 0.012s\n",
      "[241/10000] 1 tree, 31 leaves, max depth = 9, train loss: 21.00424, val loss: 28.32087, in 0.007s\n",
      "[242/10000] 1 tree, 31 leaves, max depth = 12, train loss: 20.98484, val loss: 28.19309, in 0.006s\n",
      "[243/10000] 1 tree, 31 leaves, max depth = 10, train loss: 20.97083, val loss: 28.10438, in 0.006s\n",
      "[244/10000] 1 tree, 31 leaves, max depth = 10, train loss: 20.95713, val loss: 28.01537, in 0.008s\n",
      "[245/10000] 1 tree, 31 leaves, max depth = 9, train loss: 20.94359, val loss: 27.92634, in 0.006s\n",
      "[246/10000] 1 tree, 31 leaves, max depth = 10, train loss: 20.91328, val loss: 27.83933, in 0.006s\n",
      "[247/10000] 1 tree, 31 leaves, max depth = 10, train loss: 20.89273, val loss: 27.67031, in 0.011s\n",
      "[248/10000] 1 tree, 31 leaves, max depth = 11, train loss: 20.86405, val loss: 27.58198, in 0.007s\n",
      "[249/10000] 1 tree, 31 leaves, max depth = 12, train loss: 20.84364, val loss: 27.39314, in 0.006s\n",
      "[250/10000] 1 tree, 31 leaves, max depth = 11, train loss: 20.81532, val loss: 27.30759, in 0.007s\n",
      "[251/10000] 1 tree, 31 leaves, max depth = 10, train loss: 20.79154, val loss: 27.15607, in 0.007s\n",
      "[252/10000] 1 tree, 31 leaves, max depth = 10, train loss: 20.76821, val loss: 27.00433, in 0.007s\n",
      "[253/10000] 1 tree, 31 leaves, max depth = 9, train loss: 20.74965, val loss: 26.88290, in 0.011s\n",
      "[254/10000] 1 tree, 31 leaves, max depth = 9, train loss: 20.72400, val loss: 26.69552, in 0.006s\n",
      "[255/10000] 1 tree, 31 leaves, max depth = 9, train loss: 20.70141, val loss: 26.50859, in 0.006s\n",
      "[256/10000] 1 tree, 31 leaves, max depth = 12, train loss: 20.69005, val loss: 26.48700, in 0.006s\n",
      "[257/10000] 1 tree, 31 leaves, max depth = 9, train loss: 20.67409, val loss: 26.41678, in 0.006s\n",
      "[258/10000] 1 tree, 31 leaves, max depth = 9, train loss: 20.66082, val loss: 26.25649, in 0.006s\n",
      "[259/10000] 1 tree, 31 leaves, max depth = 12, train loss: 20.65061, val loss: 26.23594, in 0.006s\n",
      "[260/10000] 1 tree, 31 leaves, max depth = 10, train loss: 20.64014, val loss: 26.21777, in 0.021s\n",
      "[261/10000] 1 tree, 31 leaves, max depth = 9, train loss: 20.62710, val loss: 26.10333, in 0.006s\n",
      "[262/10000] 1 tree, 31 leaves, max depth = 11, train loss: 20.61483, val loss: 25.98898, in 0.006s\n",
      "[263/10000] 1 tree, 31 leaves, max depth = 10, train loss: 20.60158, val loss: 25.82463, in 0.006s\n",
      "[264/10000] 1 tree, 31 leaves, max depth = 9, train loss: 20.59095, val loss: 25.77980, in 0.007s\n",
      "[265/10000] 1 tree, 31 leaves, max depth = 11, train loss: 20.57278, val loss: 25.77097, in 0.006s\n",
      "[266/10000] 1 tree, 31 leaves, max depth = 9, train loss: 20.55698, val loss: 25.70362, in 0.011s\n",
      "[267/10000] 1 tree, 31 leaves, max depth = 11, train loss: 20.54216, val loss: 25.69496, in 0.006s\n",
      "[268/10000] 1 tree, 31 leaves, max depth = 11, train loss: 20.52762, val loss: 25.68638, in 0.006s\n",
      "[269/10000] 1 tree, 31 leaves, max depth = 11, train loss: 20.51206, val loss: 25.67368, in 0.007s\n",
      "[270/10000] 1 tree, 31 leaves, max depth = 10, train loss: 20.50181, val loss: 25.63973, in 0.011s\n",
      "[271/10000] 1 tree, 31 leaves, max depth = 10, train loss: 20.48702, val loss: 25.57221, in 0.007s\n",
      "[272/10000] 1 tree, 31 leaves, max depth = 13, train loss: 20.47982, val loss: 25.53599, in 0.006s\n",
      "[273/10000] 1 tree, 31 leaves, max depth = 12, train loss: 20.46626, val loss: 25.53190, in 0.006s\n",
      "[274/10000] 1 tree, 31 leaves, max depth = 10, train loss: 20.45806, val loss: 25.51703, in 0.006s\n",
      "[275/10000] 1 tree, 31 leaves, max depth = 9, train loss: 20.44352, val loss: 25.39809, in 0.007s\n",
      "[276/10000] 1 tree, 31 leaves, max depth = 9, train loss: 20.43485, val loss: 25.38329, in 0.007s\n",
      "[277/10000] 1 tree, 31 leaves, max depth = 8, train loss: 20.42827, val loss: 25.37077, in 0.006s\n",
      "[278/10000] 1 tree, 31 leaves, max depth = 8, train loss: 20.42178, val loss: 25.35339, in 0.012s\n",
      "[279/10000] 1 tree, 31 leaves, max depth = 8, train loss: 20.40445, val loss: 25.25482, in 0.088s\n",
      "[280/10000] 1 tree, 31 leaves, max depth = 11, train loss: 20.39246, val loss: 25.18955, in 0.008s\n",
      "[281/10000] 1 tree, 31 leaves, max depth = 8, train loss: 20.37536, val loss: 25.09233, in 0.007s\n",
      "[282/10000] 1 tree, 31 leaves, max depth = 8, train loss: 20.35872, val loss: 24.99608, in 0.012s\n",
      "[283/10000] 1 tree, 31 leaves, max depth = 13, train loss: 20.33349, val loss: 24.99275, in 0.007s\n",
      "[284/10000] 1 tree, 31 leaves, max depth = 12, train loss: 20.31780, val loss: 24.99113, in 0.007s\n",
      "[285/10000] 1 tree, 31 leaves, max depth = 10, train loss: 20.30213, val loss: 24.99254, in 0.006s\n",
      "[286/10000] 1 tree, 31 leaves, max depth = 10, train loss: 20.28662, val loss: 24.99394, in 0.013s\n",
      "[287/10000] 1 tree, 31 leaves, max depth = 10, train loss: 20.27124, val loss: 24.99307, in 0.006s\n",
      "[288/10000] 1 tree, 31 leaves, max depth = 9, train loss: 20.25701, val loss: 24.98419, in 0.007s\n",
      "[289/10000] 1 tree, 31 leaves, max depth = 9, train loss: 20.23383, val loss: 24.75335, in 0.007s\n",
      "[290/10000] 1 tree, 31 leaves, max depth = 10, train loss: 20.21396, val loss: 24.58421, in 0.006s\n",
      "[291/10000] 1 tree, 31 leaves, max depth = 12, train loss: 20.20000, val loss: 24.52327, in 0.013s\n",
      "[292/10000] 1 tree, 31 leaves, max depth = 9, train loss: 20.19580, val loss: 24.49627, in 0.007s\n",
      "[293/10000] 1 tree, 31 leaves, max depth = 12, train loss: 20.18777, val loss: 24.39347, in 0.006s\n",
      "[294/10000] 1 tree, 31 leaves, max depth = 9, train loss: 20.17631, val loss: 24.32033, in 0.007s\n",
      "[295/10000] 1 tree, 31 leaves, max depth = 11, train loss: 20.16459, val loss: 24.22676, in 0.007s\n",
      "[296/10000] 1 tree, 31 leaves, max depth = 14, train loss: 20.15421, val loss: 24.12438, in 0.006s\n",
      "[297/10000] 1 tree, 31 leaves, max depth = 9, train loss: 20.13587, val loss: 23.97046, in 0.011s\n",
      "[298/10000] 1 tree, 31 leaves, max depth = 10, train loss: 20.13029, val loss: 23.96075, in 0.006s\n",
      "[299/10000] 1 tree, 31 leaves, max depth = 10, train loss: 20.12404, val loss: 23.95173, in 0.007s\n",
      "[300/10000] 1 tree, 31 leaves, max depth = 10, train loss: 20.11865, val loss: 23.94221, in 0.006s\n",
      "[301/10000] 1 tree, 31 leaves, max depth = 13, train loss: 20.11166, val loss: 23.92798, in 0.006s\n",
      "[302/10000] 1 tree, 31 leaves, max depth = 12, train loss: 20.09648, val loss: 23.92429, in 0.006s\n",
      "[303/10000] 1 tree, 31 leaves, max depth = 10, train loss: 20.07965, val loss: 23.83883, in 0.006s\n",
      "[304/10000] 1 tree, 31 leaves, max depth = 10, train loss: 20.06314, val loss: 23.75667, in 0.010s\n",
      "[305/10000] 1 tree, 31 leaves, max depth = 10, train loss: 20.05106, val loss: 23.74341, in 0.006s\n",
      "[306/10000] 1 tree, 31 leaves, max depth = 12, train loss: 20.04000, val loss: 23.73486, in 0.005s\n",
      "[307/10000] 1 tree, 31 leaves, max depth = 8, train loss: 20.02963, val loss: 23.68117, in 0.008s\n",
      "[308/10000] 1 tree, 31 leaves, max depth = 13, train loss: 20.01974, val loss: 23.67833, in 0.006s\n",
      "[309/10000] 1 tree, 31 leaves, max depth = 12, train loss: 20.00947, val loss: 23.66058, in 0.006s\n",
      "[310/10000] 1 tree, 31 leaves, max depth = 8, train loss: 19.99956, val loss: 23.60846, in 0.006s\n",
      "[311/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.99583, val loss: 23.57611, in 0.010s\n",
      "[312/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.99233, val loss: 23.55080, in 0.006s\n",
      "[313/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.98201, val loss: 23.48323, in 0.006s\n",
      "[314/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.96893, val loss: 23.47541, in 0.006s\n",
      "[315/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.96249, val loss: 23.46240, in 0.032s\n",
      "[316/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.95606, val loss: 23.44954, in 0.010s\n",
      "[317/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.94974, val loss: 23.42651, in 0.006s\n",
      "[318/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.92742, val loss: 23.17950, in 0.006s\n",
      "[319/10000] 1 tree, 31 leaves, max depth = 8, train loss: 19.91152, val loss: 23.12440, in 0.006s\n",
      "[320/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.90245, val loss: 23.07004, in 0.006s\n",
      "[321/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.89163, val loss: 22.95276, in 0.006s\n",
      "[322/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.88574, val loss: 22.95139, in 0.006s\n",
      "[323/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.87653, val loss: 22.89803, in 0.010s\n",
      "[324/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.86613, val loss: 22.78269, in 0.005s\n",
      "[325/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.86020, val loss: 22.78194, in 0.007s\n",
      "[326/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.85272, val loss: 22.72846, in 0.006s\n",
      "[327/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.84568, val loss: 22.67454, in 0.006s\n",
      "[328/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.83850, val loss: 22.62201, in 0.007s\n",
      "[329/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.83115, val loss: 22.57009, in 0.006s\n",
      "[330/10000] 1 tree, 31 leaves, max depth = 13, train loss: 19.82879, val loss: 22.55962, in 0.006s\n",
      "[331/10000] 1 tree, 31 leaves, max depth = 13, train loss: 19.82228, val loss: 22.55381, in 0.006s\n",
      "[332/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.81505, val loss: 22.55025, in 0.012s\n",
      "[333/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.80897, val loss: 22.54639, in 0.005s\n",
      "[334/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.80251, val loss: 22.54332, in 0.006s\n",
      "[335/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.79079, val loss: 22.41318, in 0.007s\n",
      "[336/10000] 1 tree, 31 leaves, max depth = 13, train loss: 19.78746, val loss: 22.39455, in 0.006s\n",
      "[337/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.78218, val loss: 22.39177, in 0.006s\n",
      "[338/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.77239, val loss: 22.38349, in 0.006s\n",
      "[339/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.76523, val loss: 22.38249, in 0.011s\n",
      "[340/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.76236, val loss: 22.37179, in 0.006s\n",
      "[341/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.74789, val loss: 22.15829, in 0.006s\n",
      "[342/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.73690, val loss: 22.03255, in 0.006s\n",
      "[343/10000] 1 tree, 31 leaves, max depth = 8, train loss: 19.72179, val loss: 21.78478, in 0.006s\n",
      "[344/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.71653, val loss: 21.77473, in 0.006s\n",
      "[345/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.71379, val loss: 21.76703, in 0.011s\n",
      "[346/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.70025, val loss: 21.56181, in 0.006s\n",
      "[347/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.68771, val loss: 21.35861, in 0.006s\n",
      "[348/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.67785, val loss: 21.24193, in 0.082s\n",
      "[349/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.67295, val loss: 21.20707, in 0.006s\n",
      "[350/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.65636, val loss: 21.20443, in 0.006s\n",
      "[351/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.65292, val loss: 21.17815, in 0.005s\n",
      "[352/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.64723, val loss: 21.17374, in 0.011s\n",
      "[353/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.64213, val loss: 21.16859, in 0.006s\n",
      "[354/10000] 1 tree, 31 leaves, max depth = 8, train loss: 19.63401, val loss: 21.16810, in 0.006s\n",
      "[355/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.63025, val loss: 21.15016, in 0.006s\n",
      "[356/10000] 1 tree, 31 leaves, max depth = 8, train loss: 19.62220, val loss: 21.14956, in 0.006s\n",
      "[357/10000] 1 tree, 31 leaves, max depth = 8, train loss: 19.60617, val loss: 21.14894, in 0.005s\n",
      "[358/10000] 1 tree, 31 leaves, max depth = 8, train loss: 19.59822, val loss: 21.14847, in 0.005s\n",
      "[359/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.59483, val loss: 21.12574, in 0.012s\n",
      "[360/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.59069, val loss: 21.09139, in 0.006s\n",
      "[361/10000] 1 tree, 31 leaves, max depth = 8, train loss: 19.58305, val loss: 21.09079, in 0.006s\n",
      "[362/10000] 1 tree, 31 leaves, max depth = 7, train loss: 19.57584, val loss: 21.09020, in 0.005s\n",
      "[363/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.56954, val loss: 21.08974, in 0.006s\n",
      "[364/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.56522, val loss: 21.06457, in 0.005s\n",
      "[365/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.56292, val loss: 21.05687, in 0.006s\n",
      "[366/10000] 1 tree, 31 leaves, max depth = 7, train loss: 19.56001, val loss: 21.05647, in 0.011s\n",
      "[367/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.54941, val loss: 20.97438, in 0.006s\n",
      "[368/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.53934, val loss: 20.89312, in 0.005s\n",
      "[369/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.52738, val loss: 20.68103, in 0.006s\n",
      "[370/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.52053, val loss: 20.64887, in 0.005s\n",
      "[371/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.50771, val loss: 20.48390, in 0.006s\n",
      "[372/10000] 1 tree, 31 leaves, max depth = 8, train loss: 19.50382, val loss: 20.48338, in 0.005s\n",
      "[373/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.49757, val loss: 20.43569, in 0.005s\n",
      "[374/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.49247, val loss: 20.43519, in 0.011s\n",
      "[375/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.48100, val loss: 20.43485, in 0.120s\n",
      "[376/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.47548, val loss: 20.43451, in 0.007s\n",
      "[377/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.46888, val loss: 20.38733, in 0.006s\n",
      "[378/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.46653, val loss: 20.35880, in 0.007s\n",
      "[379/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.46514, val loss: 20.34132, in 0.035s\n",
      "[380/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.46378, val loss: 20.32402, in 0.006s\n",
      "[381/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.46196, val loss: 20.29753, in 0.006s\n",
      "[382/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.45933, val loss: 20.29540, in 0.006s\n",
      "[383/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.45673, val loss: 20.29330, in 0.012s\n",
      "[384/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.45415, val loss: 20.29119, in 0.008s\n",
      "[385/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.44836, val loss: 20.27072, in 0.006s\n",
      "[386/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.44686, val loss: 20.25862, in 0.007s\n",
      "[387/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.44326, val loss: 20.24844, in 0.006s\n",
      "[388/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.44300, val loss: 20.24799, in 0.006s\n",
      "[389/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.43665, val loss: 20.24241, in 0.012s\n",
      "[390/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.43640, val loss: 20.24197, in 0.043s\n",
      "[391/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.43610, val loss: 20.24158, in 0.007s\n",
      "[392/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.43456, val loss: 20.22221, in 0.006s\n",
      "[393/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.43304, val loss: 20.20304, in 0.007s\n",
      "[394/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.43156, val loss: 20.19290, in 0.011s\n",
      "[395/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.42970, val loss: 20.18058, in 0.006s\n",
      "[396/10000] 1 tree, 31 leaves, max depth = 13, train loss: 19.42943, val loss: 20.18010, in 0.006s\n",
      "[397/10000] 1 tree, 31 leaves, max depth = 13, train loss: 19.42917, val loss: 20.17963, in 0.006s\n",
      "[398/10000] 1 tree, 31 leaves, max depth = 13, train loss: 19.42891, val loss: 20.17917, in 0.006s\n",
      "[399/10000] 1 tree, 31 leaves, max depth = 13, train loss: 19.42866, val loss: 20.17870, in 0.006s\n",
      "[400/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.42438, val loss: 20.13136, in 0.006s\n",
      "[401/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.42136, val loss: 20.11100, in 0.005s\n",
      "[402/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.41953, val loss: 20.10108, in 0.011s\n",
      "[403/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.41807, val loss: 20.09843, in 0.005s\n",
      "[404/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.41662, val loss: 20.09580, in 0.006s\n",
      "[405/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.41520, val loss: 20.09320, in 0.012s\n",
      "[406/10000] 1 tree, 31 leaves, max depth = 13, train loss: 19.41496, val loss: 20.09277, in 0.006s\n",
      "[407/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.41390, val loss: 20.08930, in 0.007s\n",
      "[408/10000] 1 tree, 31 leaves, max depth = 13, train loss: 19.41366, val loss: 20.08887, in 0.011s\n",
      "[409/10000] 1 tree, 31 leaves, max depth = 13, train loss: 19.41343, val loss: 20.08845, in 0.006s\n",
      "[410/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.40221, val loss: 19.89564, in 0.006s\n",
      "[411/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.39109, val loss: 19.70477, in 0.006s\n",
      "[412/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.38057, val loss: 19.51491, in 0.006s\n",
      "[413/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.38032, val loss: 19.51456, in 0.006s\n",
      "[414/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.38012, val loss: 19.51421, in 0.007s\n",
      "[415/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.37991, val loss: 19.51386, in 0.005s\n",
      "[416/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.37971, val loss: 19.51352, in 0.005s\n",
      "[417/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.37888, val loss: 19.51117, in 0.012s\n",
      "[418/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.37850, val loss: 19.50959, in 0.006s\n",
      "[419/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.36943, val loss: 19.34734, in 0.006s\n",
      "[420/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.36907, val loss: 19.34577, in 0.049s\n",
      "[421/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.36888, val loss: 19.34544, in 0.007s\n",
      "[422/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.36870, val loss: 19.34512, in 0.006s\n",
      "[423/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.36838, val loss: 19.34360, in 0.006s\n",
      "[424/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.36820, val loss: 19.34327, in 0.006s\n",
      "[425/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.36566, val loss: 19.33177, in 0.012s\n",
      "[426/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.36504, val loss: 19.32716, in 0.006s\n",
      "[427/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.36428, val loss: 19.32550, in 0.006s\n",
      "[428/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.36303, val loss: 19.32591, in 0.006s\n",
      "[429/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.36004, val loss: 19.29393, in 0.006s\n",
      "[430/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.35479, val loss: 19.19227, in 0.007s\n",
      "[431/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.35457, val loss: 19.19070, in 0.006s\n",
      "[432/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.35439, val loss: 19.18980, in 0.005s\n",
      "[433/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.35415, val loss: 19.18664, in 0.012s\n",
      "[434/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.35399, val loss: 19.18642, in 0.006s\n",
      "[435/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.35383, val loss: 19.18615, in 0.006s\n",
      "[436/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.35361, val loss: 19.18461, in 0.006s\n",
      "[437/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.35346, val loss: 19.18435, in 0.007s\n",
      "[438/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.35325, val loss: 19.18283, in 0.006s\n",
      "[439/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.35309, val loss: 19.18258, in 0.006s\n",
      "[440/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.35294, val loss: 19.18231, in 0.020s\n",
      "[441/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.35277, val loss: 19.18158, in 0.007s\n",
      "[442/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.35262, val loss: 19.18133, in 0.007s\n",
      "[443/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.35246, val loss: 19.18062, in 0.006s\n",
      "[444/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.35232, val loss: 19.18038, in 0.007s\n",
      "[445/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.35217, val loss: 19.18018, in 0.017s\n",
      "[446/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.35203, val loss: 19.17994, in 0.005s\n",
      "[447/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.35189, val loss: 19.17968, in 0.006s\n",
      "[448/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.35169, val loss: 19.17821, in 0.005s\n",
      "[449/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.35148, val loss: 19.17580, in 0.006s\n",
      "[450/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.35129, val loss: 19.17434, in 0.080s\n",
      "[451/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.35115, val loss: 19.17411, in 0.008s\n",
      "[452/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.35071, val loss: 19.16972, in 0.006s\n",
      "[453/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.35043, val loss: 19.16736, in 0.006s\n",
      "[454/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.35030, val loss: 19.16714, in 0.013s\n",
      "[455/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.34869, val loss: 19.15219, in 0.006s\n",
      "[456/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.34821, val loss: 19.13787, in 0.006s\n",
      "[457/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.34808, val loss: 19.13765, in 0.006s\n",
      "[458/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.34788, val loss: 19.13674, in 0.006s\n",
      "[459/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.34771, val loss: 19.13573, in 0.005s\n",
      "[460/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.34759, val loss: 19.13552, in 0.012s\n",
      "[461/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.34639, val loss: 19.12286, in 0.006s\n",
      "[462/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.33485, val loss: 19.01217, in 0.006s\n",
      "[463/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.32520, val loss: 18.82847, in 0.005s\n",
      "[464/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.31565, val loss: 18.64660, in 0.006s\n",
      "[465/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.30655, val loss: 18.46654, in 0.006s\n",
      "[466/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.30642, val loss: 18.46626, in 0.007s\n",
      "[467/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.30629, val loss: 18.46598, in 0.013s\n",
      "[468/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.30586, val loss: 18.46200, in 0.006s\n",
      "[469/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.30470, val loss: 18.44706, in 0.006s\n",
      "[470/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.30355, val loss: 18.43228, in 0.006s\n",
      "[471/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.30343, val loss: 18.43201, in 0.007s\n",
      "[472/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.30066, val loss: 18.39734, in 0.006s\n",
      "[473/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.29797, val loss: 18.36303, in 0.006s\n",
      "[474/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.29021, val loss: 18.23361, in 0.030s\n",
      "[475/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.28253, val loss: 18.10547, in 0.006s\n",
      "[476/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.28241, val loss: 18.10522, in 0.007s\n",
      "[477/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.27481, val loss: 17.97837, in 0.007s\n",
      "[478/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.26728, val loss: 17.85279, in 0.006s\n",
      "[479/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.25983, val loss: 17.72847, in 0.028s\n",
      "[480/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.25261, val loss: 17.60539, in 0.007s\n",
      "[481/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.25250, val loss: 17.60514, in 0.007s\n",
      "[482/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.25238, val loss: 17.60494, in 0.006s\n",
      "[483/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.25227, val loss: 17.60476, in 0.013s\n",
      "[484/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.25216, val loss: 17.60458, in 0.006s\n",
      "[485/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.25125, val loss: 17.59348, in 0.006s\n",
      "[486/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.25035, val loss: 17.58248, in 0.006s\n",
      "[487/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.25012, val loss: 17.58114, in 0.007s\n",
      "[488/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.24985, val loss: 17.57976, in 0.006s\n",
      "[489/10000] 1 tree, 31 leaves, max depth = 8, train loss: 19.24952, val loss: 17.57825, in 0.006s\n",
      "[490/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.24780, val loss: 17.55769, in 0.013s\n",
      "[491/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.24712, val loss: 17.55745, in 0.006s\n",
      "[492/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.24539, val loss: 17.53484, in 0.006s\n",
      "[493/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.24516, val loss: 17.53407, in 0.007s\n",
      "[494/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.24499, val loss: 17.53284, in 0.006s\n",
      "[495/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.23617, val loss: 17.34542, in 0.007s\n",
      "[496/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23607, val loss: 17.34520, in 0.013s\n",
      "[497/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.23597, val loss: 17.34503, in 0.006s\n",
      "[498/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.23587, val loss: 17.34485, in 0.006s\n",
      "[499/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.23578, val loss: 17.34468, in 0.006s\n",
      "[500/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.23568, val loss: 17.34451, in 0.007s\n",
      "[501/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.23559, val loss: 17.34434, in 0.009s\n",
      "[502/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.23550, val loss: 17.34417, in 0.012s\n",
      "[503/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.23541, val loss: 17.34401, in 0.006s\n",
      "[504/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.23532, val loss: 17.34387, in 0.006s\n",
      "[505/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.23524, val loss: 17.34373, in 0.007s\n",
      "[506/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.23516, val loss: 17.34359, in 0.007s\n",
      "[507/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.23507, val loss: 17.34345, in 0.006s\n",
      "[508/10000] 1 tree, 31 leaves, max depth = 13, train loss: 19.23499, val loss: 17.34331, in 0.007s\n",
      "[509/10000] 1 tree, 31 leaves, max depth = 13, train loss: 19.23491, val loss: 17.34318, in 0.014s\n",
      "[510/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.23483, val loss: 17.34305, in 0.006s\n",
      "[511/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.23474, val loss: 17.34286, in 0.006s\n",
      "[512/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.23466, val loss: 17.34267, in 0.008s\n",
      "[513/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.23458, val loss: 17.34248, in 0.006s\n",
      "[514/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.23449, val loss: 17.34230, in 0.006s\n",
      "[515/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.23441, val loss: 17.34211, in 0.013s\n",
      "[516/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.23433, val loss: 17.34193, in 0.006s\n",
      "[517/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.23425, val loss: 17.34175, in 0.005s\n",
      "[518/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.23417, val loss: 17.34157, in 0.007s\n",
      "[519/10000] 1 tree, 31 leaves, max depth = 14, train loss: 19.23409, val loss: 17.34143, in 0.006s\n",
      "[520/10000] 1 tree, 31 leaves, max depth = 14, train loss: 19.23402, val loss: 17.34129, in 0.006s\n",
      "[521/10000] 1 tree, 31 leaves, max depth = 14, train loss: 19.23395, val loss: 17.34114, in 0.012s\n",
      "[522/10000] 1 tree, 31 leaves, max depth = 14, train loss: 19.23387, val loss: 17.34085, in 0.006s\n",
      "[523/10000] 1 tree, 31 leaves, max depth = 14, train loss: 19.23380, val loss: 17.34071, in 0.005s\n",
      "[524/10000] 1 tree, 31 leaves, max depth = 14, train loss: 19.23373, val loss: 17.34058, in 0.006s\n",
      "[525/10000] 1 tree, 31 leaves, max depth = 14, train loss: 19.23366, val loss: 17.34044, in 0.006s\n",
      "[526/10000] 1 tree, 31 leaves, max depth = 14, train loss: 19.23360, val loss: 17.34031, in 0.007s\n",
      "[527/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23352, val loss: 17.34018, in 0.006s\n",
      "[528/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23344, val loss: 17.34005, in 0.013s\n",
      "[529/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23337, val loss: 17.33992, in 0.006s\n",
      "[530/10000] 1 tree, 31 leaves, max depth = 14, train loss: 19.23330, val loss: 17.33980, in 0.006s\n",
      "[531/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23322, val loss: 17.33967, in 0.007s\n",
      "[532/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23315, val loss: 17.33954, in 0.006s\n",
      "[533/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23308, val loss: 17.33942, in 0.006s\n",
      "[534/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.23301, val loss: 17.33929, in 0.006s\n",
      "[535/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23295, val loss: 17.33920, in 0.013s\n",
      "[536/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.23288, val loss: 17.33907, in 0.014s\n",
      "[537/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.23282, val loss: 17.33895, in 0.006s\n",
      "[538/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.23276, val loss: 17.33883, in 0.007s\n",
      "[539/10000] 1 tree, 31 leaves, max depth = 13, train loss: 19.23268, val loss: 17.33872, in 0.006s\n",
      "[540/10000] 1 tree, 31 leaves, max depth = 13, train loss: 19.23261, val loss: 17.33860, in 0.006s\n",
      "[541/10000] 1 tree, 31 leaves, max depth = 13, train loss: 19.23255, val loss: 17.33848, in 0.014s\n",
      "[542/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23248, val loss: 17.33837, in 0.006s\n",
      "[543/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23241, val loss: 17.33826, in 0.006s\n",
      "[544/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23235, val loss: 17.33814, in 0.006s\n",
      "[545/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23228, val loss: 17.33832, in 0.006s\n",
      "[546/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23222, val loss: 17.33821, in 0.007s\n",
      "[547/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23216, val loss: 17.33810, in 0.006s\n",
      "[548/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23210, val loss: 17.33800, in 0.013s\n",
      "[549/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23205, val loss: 17.33789, in 0.006s\n",
      "[550/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23199, val loss: 17.33779, in 0.006s\n",
      "[551/10000] 1 tree, 31 leaves, max depth = 13, train loss: 19.23193, val loss: 17.33768, in 0.006s\n",
      "[552/10000] 1 tree, 31 leaves, max depth = 13, train loss: 19.23188, val loss: 17.33758, in 0.006s\n",
      "[553/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23182, val loss: 17.33748, in 0.006s\n",
      "[554/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23177, val loss: 17.33766, in 0.006s\n",
      "[555/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.23171, val loss: 17.33756, in 0.013s\n",
      "[556/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.23166, val loss: 17.33746, in 0.006s\n",
      "[557/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23161, val loss: 17.33765, in 0.006s\n",
      "[558/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23156, val loss: 17.33783, in 0.005s\n",
      "[559/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.23151, val loss: 17.33774, in 0.006s\n",
      "[560/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23146, val loss: 17.33764, in 0.006s\n",
      "[561/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23141, val loss: 17.33755, in 0.014s\n",
      "[562/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23136, val loss: 17.33745, in 0.006s\n",
      "[563/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23131, val loss: 17.33736, in 0.006s\n",
      "[564/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23126, val loss: 17.33727, in 0.033s\n",
      "[565/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23122, val loss: 17.33718, in 0.007s\n",
      "[566/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23117, val loss: 17.33709, in 0.013s\n",
      "[567/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23113, val loss: 17.33700, in 0.006s\n",
      "[568/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23108, val loss: 17.33691, in 0.025s\n",
      "[569/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23103, val loss: 17.33682, in 0.008s\n",
      "[570/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23099, val loss: 17.33674, in 0.006s\n",
      "[571/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23094, val loss: 17.33665, in 0.005s\n",
      "[572/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23090, val loss: 17.33657, in 0.012s\n",
      "[573/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23086, val loss: 17.33648, in 0.006s\n",
      "[574/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23082, val loss: 17.33640, in 0.006s\n",
      "[575/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23077, val loss: 17.33631, in 0.006s\n",
      "[576/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23073, val loss: 17.33622, in 0.007s\n",
      "[577/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23069, val loss: 17.33614, in 0.006s\n",
      "[578/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23064, val loss: 17.33605, in 0.006s\n",
      "[579/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23060, val loss: 17.33596, in 0.013s\n",
      "[580/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23056, val loss: 17.33589, in 0.006s\n",
      "[581/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23052, val loss: 17.33581, in 0.006s\n",
      "[582/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.23048, val loss: 17.33573, in 0.006s\n",
      "[583/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23044, val loss: 17.33566, in 0.008s\n",
      "[584/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23040, val loss: 17.33558, in 0.006s\n",
      "[585/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23037, val loss: 17.33551, in 0.006s\n",
      "[586/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23033, val loss: 17.33543, in 0.013s\n",
      "[587/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23029, val loss: 17.33536, in 0.006s\n",
      "[588/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23025, val loss: 17.33529, in 0.006s\n",
      "[589/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23021, val loss: 17.33522, in 0.006s\n",
      "[590/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23018, val loss: 17.33515, in 0.006s\n",
      "[591/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23014, val loss: 17.33507, in 0.005s\n",
      "[592/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23010, val loss: 17.33500, in 0.013s\n",
      "[593/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23007, val loss: 17.33492, in 0.006s\n",
      "[594/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23003, val loss: 17.33484, in 0.006s\n",
      "[595/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.23000, val loss: 17.33477, in 0.006s\n",
      "[596/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.22996, val loss: 17.33470, in 0.006s\n",
      "[597/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22993, val loss: 17.33491, in 0.007s\n",
      "[598/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22979, val loss: 17.33373, in 0.025s\n",
      "[599/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22966, val loss: 17.33255, in 0.006s\n",
      "[600/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22960, val loss: 17.33181, in 0.007s\n",
      "[601/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22886, val loss: 17.32176, in 0.006s\n",
      "[602/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.22883, val loss: 17.32170, in 0.007s\n",
      "[603/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22879, val loss: 17.32164, in 0.013s\n",
      "[604/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.22876, val loss: 17.32157, in 0.006s\n",
      "[605/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22873, val loss: 17.32151, in 0.006s\n",
      "[606/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22869, val loss: 17.32145, in 0.006s\n",
      "[607/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22866, val loss: 17.32139, in 0.006s\n",
      "[608/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22862, val loss: 17.32161, in 0.006s\n",
      "[609/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22859, val loss: 17.32155, in 0.006s\n",
      "[610/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22856, val loss: 17.32150, in 0.006s\n",
      "[611/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22854, val loss: 17.32144, in 0.006s\n",
      "[612/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22851, val loss: 17.32138, in 0.069s\n",
      "[613/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.22848, val loss: 17.32133, in 0.006s\n",
      "[614/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22845, val loss: 17.32127, in 0.007s\n",
      "[615/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22842, val loss: 17.32122, in 0.006s\n",
      "[616/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22839, val loss: 17.32116, in 0.005s\n",
      "[617/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22837, val loss: 17.32111, in 0.044s\n",
      "[618/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22834, val loss: 17.32105, in 0.016s\n",
      "[619/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22831, val loss: 17.32100, in 0.007s\n",
      "[620/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22828, val loss: 17.32095, in 0.006s\n",
      "[621/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22826, val loss: 17.32090, in 0.008s\n",
      "[622/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22823, val loss: 17.32085, in 0.006s\n",
      "[623/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22820, val loss: 17.32079, in 0.007s\n",
      "[624/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.22818, val loss: 17.32074, in 0.017s\n",
      "[625/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22815, val loss: 17.32069, in 0.007s\n",
      "[626/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22812, val loss: 17.32064, in 0.008s\n",
      "[627/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22809, val loss: 17.32059, in 0.007s\n",
      "[628/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22806, val loss: 17.32054, in 0.035s\n",
      "[629/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22803, val loss: 17.32049, in 0.006s\n",
      "[630/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22801, val loss: 17.32044, in 0.006s\n",
      "[631/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22798, val loss: 17.32039, in 0.006s\n",
      "[632/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22795, val loss: 17.32034, in 0.016s\n",
      "[633/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22793, val loss: 17.32029, in 0.006s\n",
      "[634/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22790, val loss: 17.32025, in 0.006s\n",
      "[635/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.22788, val loss: 17.32020, in 0.006s\n",
      "[636/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22785, val loss: 17.32015, in 0.006s\n",
      "[637/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22555, val loss: 17.31308, in 0.006s\n",
      "[638/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22552, val loss: 17.31303, in 0.006s\n",
      "[639/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22325, val loss: 17.30603, in 0.015s\n",
      "[640/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22317, val loss: 17.30599, in 0.006s\n",
      "[641/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.22312, val loss: 17.30527, in 0.007s\n",
      "[642/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22304, val loss: 17.30435, in 0.006s\n",
      "[643/10000] 1 tree, 31 leaves, max depth = 13, train loss: 19.22302, val loss: 17.30431, in 0.006s\n",
      "[644/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22299, val loss: 17.30427, in 0.013s\n",
      "[645/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22297, val loss: 17.30422, in 0.038s\n",
      "[646/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.22294, val loss: 17.30418, in 0.024s\n",
      "[647/10000] 1 tree, 31 leaves, max depth = 13, train loss: 19.22292, val loss: 17.30414, in 0.006s\n",
      "[648/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22289, val loss: 17.30410, in 0.005s\n",
      "[649/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22287, val loss: 17.30406, in 0.005s\n",
      "[650/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22285, val loss: 17.30402, in 0.006s\n",
      "[651/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22283, val loss: 17.30398, in 0.022s\n",
      "[652/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22280, val loss: 17.30394, in 0.007s\n",
      "[653/10000] 1 tree, 31 leaves, max depth = 13, train loss: 19.22278, val loss: 17.30390, in 0.006s\n",
      "[654/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22276, val loss: 17.30386, in 0.005s\n",
      "[655/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.22274, val loss: 17.30383, in 0.013s\n",
      "[656/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22271, val loss: 17.30379, in 0.006s\n",
      "[657/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22269, val loss: 17.30375, in 0.007s\n",
      "[658/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22267, val loss: 17.30371, in 0.005s\n",
      "[659/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22265, val loss: 17.30368, in 0.007s\n",
      "[660/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22263, val loss: 17.30364, in 0.005s\n",
      "[661/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22261, val loss: 17.30361, in 0.006s\n",
      "[662/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22259, val loss: 17.30357, in 0.014s\n",
      "[663/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22257, val loss: 17.30354, in 0.005s\n",
      "[664/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22254, val loss: 17.30350, in 0.006s\n",
      "[665/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22252, val loss: 17.30347, in 0.007s\n",
      "[666/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22250, val loss: 17.30343, in 0.007s\n",
      "[667/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22248, val loss: 17.30340, in 0.006s\n",
      "[668/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22246, val loss: 17.30337, in 0.013s\n",
      "[669/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22240, val loss: 17.30262, in 0.006s\n",
      "[670/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22228, val loss: 17.30167, in 0.005s\n",
      "[671/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22226, val loss: 17.30164, in 0.007s\n",
      "[672/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.22224, val loss: 17.30161, in 0.006s\n",
      "[673/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.22222, val loss: 17.30158, in 0.006s\n",
      "[674/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22152, val loss: 17.29181, in 0.005s\n",
      "[675/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22150, val loss: 17.29178, in 0.005s\n",
      "[676/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22148, val loss: 17.29175, in 0.072s\n",
      "[677/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22146, val loss: 17.29172, in 0.007s\n",
      "[678/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22145, val loss: 17.29169, in 0.007s\n",
      "[679/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22143, val loss: 17.29166, in 0.005s\n",
      "[680/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.22141, val loss: 17.29163, in 0.045s\n",
      "[681/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.22139, val loss: 17.29160, in 0.005s\n",
      "[682/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.22137, val loss: 17.29167, in 0.018s\n",
      "[683/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.22136, val loss: 17.29164, in 0.006s\n",
      "[684/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.22134, val loss: 17.29162, in 0.006s\n",
      "[685/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22132, val loss: 17.29159, in 0.053s\n",
      "[686/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22131, val loss: 17.29156, in 0.017s\n",
      "[687/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22129, val loss: 17.29153, in 0.015s\n",
      "[688/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22127, val loss: 17.29151, in 0.006s\n",
      "[689/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22126, val loss: 17.29148, in 0.006s\n",
      "[690/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22125, val loss: 17.29145, in 0.006s\n",
      "[691/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22123, val loss: 17.29142, in 0.007s\n",
      "[692/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22122, val loss: 17.29140, in 0.007s\n",
      "[693/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22120, val loss: 17.29137, in 0.006s\n",
      "[694/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22119, val loss: 17.29135, in 0.005s\n",
      "[695/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22117, val loss: 17.29133, in 0.014s\n",
      "[696/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.22115, val loss: 17.29130, in 0.006s\n",
      "[697/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22114, val loss: 17.29128, in 0.006s\n",
      "[698/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22112, val loss: 17.29126, in 0.007s\n",
      "[699/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.22111, val loss: 17.29124, in 0.006s\n",
      "[700/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22109, val loss: 17.29121, in 0.006s\n",
      "[701/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.22107, val loss: 17.29119, in 0.013s\n",
      "[702/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.22105, val loss: 17.29117, in 0.006s\n",
      "[703/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.22104, val loss: 17.29115, in 0.006s\n",
      "[704/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22102, val loss: 17.29113, in 0.007s\n",
      "[705/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22100, val loss: 17.29111, in 0.005s\n",
      "[706/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.22099, val loss: 17.29109, in 0.006s\n",
      "[707/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.22097, val loss: 17.29107, in 0.013s\n",
      "[708/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.22096, val loss: 17.29104, in 0.006s\n",
      "[709/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.22094, val loss: 17.29102, in 0.005s\n",
      "[710/10000] 1 tree, 31 leaves, max depth = 13, train loss: 19.22093, val loss: 17.29100, in 0.007s\n",
      "[711/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22091, val loss: 17.29098, in 0.006s\n",
      "[712/10000] 1 tree, 31 leaves, max depth = 13, train loss: 19.22090, val loss: 17.29096, in 0.005s\n",
      "[713/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.22087, val loss: 17.29094, in 0.014s\n",
      "[714/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.22086, val loss: 17.29092, in 0.006s\n",
      "[715/10000] 1 tree, 31 leaves, max depth = 13, train loss: 19.22085, val loss: 17.29090, in 0.006s\n",
      "[716/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.22083, val loss: 17.29088, in 0.006s\n",
      "[717/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.20918, val loss: 17.29087, in 0.008s\n",
      "[718/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.20916, val loss: 17.29085, in 0.006s\n",
      "[719/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.20914, val loss: 17.29083, in 0.005s\n",
      "[720/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.20913, val loss: 17.29081, in 0.014s\n",
      "[721/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.20912, val loss: 17.29079, in 0.007s\n",
      "[722/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.20783, val loss: 17.29078, in 0.005s\n",
      "[723/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.20781, val loss: 17.29077, in 0.006s\n",
      "[724/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.20780, val loss: 17.29075, in 0.007s\n",
      "[725/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.20778, val loss: 17.29074, in 0.014s\n",
      "[726/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.20770, val loss: 17.28969, in 0.006s\n",
      "[727/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.20769, val loss: 17.28967, in 0.006s\n",
      "[728/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.20767, val loss: 17.28965, in 0.006s\n",
      "[729/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.20766, val loss: 17.28964, in 0.007s\n",
      "[730/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.20765, val loss: 17.28962, in 0.006s\n",
      "[731/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.20764, val loss: 17.28960, in 0.006s\n",
      "[732/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.20762, val loss: 17.28959, in 0.074s\n",
      "[733/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.20674, val loss: 17.28955, in 0.007s\n",
      "[734/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.20568, val loss: 17.28956, in 0.007s\n",
      "[735/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.20460, val loss: 17.28968, in 0.009s\n",
      "[736/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.20457, val loss: 17.28966, in 0.016s\n",
      "[737/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.20366, val loss: 17.28849, in 0.007s\n",
      "[738/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.20188, val loss: 17.28484, in 0.030s\n",
      "[739/10000] 1 tree, 31 leaves, max depth = 8, train loss: 19.19994, val loss: 17.28125, in 0.020s\n",
      "[740/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.19250, val loss: 17.28103, in 0.007s\n",
      "[741/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.19174, val loss: 17.28119, in 0.006s\n",
      "[742/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.19022, val loss: 17.27184, in 0.017s\n",
      "[743/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.18871, val loss: 17.26332, in 0.006s\n",
      "[744/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.18724, val loss: 17.25407, in 0.008s\n",
      "[745/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.18579, val loss: 17.24492, in 0.006s\n",
      "[746/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.18436, val loss: 17.23585, in 0.006s\n",
      "[747/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.18292, val loss: 17.22720, in 0.005s\n",
      "[748/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.18152, val loss: 17.21832, in 0.028s\n",
      "[749/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.18016, val loss: 17.20908, in 0.007s\n",
      "[750/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.17840, val loss: 17.19680, in 0.006s\n",
      "[751/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.17043, val loss: 17.04927, in 0.007s\n",
      "[752/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.16259, val loss: 16.90391, in 0.023s\n",
      "[753/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.15508, val loss: 16.75969, in 0.006s\n",
      "[754/10000] 1 tree, 31 leaves, max depth = 13, train loss: 19.14691, val loss: 16.61284, in 0.006s\n",
      "[755/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.14551, val loss: 16.61283, in 0.006s\n",
      "[756/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.14487, val loss: 16.60940, in 0.027s\n",
      "[757/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.14343, val loss: 16.60940, in 0.006s\n",
      "[758/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.13862, val loss: 16.60777, in 0.006s\n",
      "[759/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.13743, val loss: 16.60352, in 0.005s\n",
      "[760/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.13273, val loss: 16.42170, in 0.007s\n",
      "[761/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.13172, val loss: 16.42326, in 0.008s\n",
      "[762/10000] 1 tree, 31 leaves, max depth = 13, train loss: 19.13140, val loss: 16.42200, in 0.006s\n",
      "[763/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.13041, val loss: 16.42345, in 0.016s\n",
      "[764/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.12901, val loss: 16.45222, in 0.006s\n",
      "[765/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.12761, val loss: 16.43994, in 0.006s\n",
      "[766/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.12744, val loss: 16.43994, in 0.006s\n",
      "[767/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.12618, val loss: 16.41669, in 0.006s\n",
      "[768/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.12547, val loss: 16.41324, in 0.006s\n",
      "[769/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.12472, val loss: 16.41378, in 0.059s\n",
      "[770/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.12397, val loss: 16.41383, in 0.007s\n",
      "[771/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.11882, val loss: 16.41383, in 0.006s\n",
      "[772/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.11728, val loss: 16.41402, in 0.007s\n",
      "[773/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.11394, val loss: 16.40295, in 0.017s\n",
      "[774/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.11217, val loss: 16.40379, in 0.006s\n",
      "[775/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.10677, val loss: 16.39292, in 0.006s\n",
      "[776/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.10610, val loss: 16.39418, in 0.007s\n",
      "[777/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.10577, val loss: 16.39429, in 0.005s\n",
      "[778/10000] 1 tree, 31 leaves, max depth = 12, train loss: 19.09889, val loss: 16.38983, in 0.006s\n",
      "[779/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.09749, val loss: 16.39266, in 0.013s\n",
      "[780/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.09748, val loss: 16.39266, in 0.006s\n",
      "[781/10000] 1 tree, 31 leaves, max depth = 8, train loss: 19.09731, val loss: 16.38994, in 0.006s\n",
      "[782/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.09265, val loss: 16.38692, in 0.007s\n",
      "[783/10000] 1 tree, 31 leaves, max depth = 8, train loss: 19.08761, val loss: 16.38427, in 0.006s\n",
      "[784/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.08740, val loss: 16.38298, in 0.005s\n",
      "[785/10000] 1 tree, 31 leaves, max depth = 8, train loss: 19.08724, val loss: 16.38171, in 0.013s\n",
      "[786/10000] 1 tree, 31 leaves, max depth = 8, train loss: 19.08716, val loss: 16.38171, in 0.006s\n",
      "[787/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.08711, val loss: 16.38170, in 0.006s\n",
      "[788/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.08673, val loss: 16.38170, in 0.006s\n",
      "[789/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.08669, val loss: 16.38170, in 0.006s\n",
      "[790/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.08666, val loss: 16.38171, in 0.006s\n",
      "[791/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.08662, val loss: 16.38170, in 0.057s\n",
      "[792/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.08658, val loss: 16.38169, in 0.007s\n",
      "[793/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.08635, val loss: 16.37905, in 0.006s\n",
      "[794/10000] 1 tree, 31 leaves, max depth = 10, train loss: 19.08245, val loss: 16.35688, in 0.006s\n",
      "[795/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.08241, val loss: 16.35727, in 0.057s\n",
      "[796/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.08237, val loss: 16.35765, in 0.010s\n",
      "[797/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.08234, val loss: 16.35766, in 0.006s\n",
      "[798/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.08232, val loss: 16.35765, in 0.007s\n",
      "[799/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.08229, val loss: 16.35765, in 0.006s\n",
      "[800/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.08226, val loss: 16.35766, in 0.005s\n",
      "[801/10000] 1 tree, 31 leaves, max depth = 11, train loss: 19.08224, val loss: 16.35765, in 0.017s\n",
      "[802/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.08222, val loss: 16.35760, in 0.005s\n",
      "[803/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.08219, val loss: 16.35754, in 0.006s\n",
      "[804/10000] 1 tree, 31 leaves, max depth = 9, train loss: 19.08217, val loss: 16.35749, in 0.007s\n",
      "Fit 804 trees in 7.789 s, (24924 total leaves)\n",
      "Time spent computing histograms: 1.774s\n",
      "Time spent finding best splits:  1.234s\n",
      "Time spent applying splits:      1.516s\n",
      "Time spent predicting:           0.222s\n"
     ]
    },
    {
     "data": {
      "text/plain": "HistGradientBoostingRegressor(l2_regularization=0.02, learning_rate=0.01,\n                              loss='absolute_error', max_bins=100,\n                              max_iter=10000, min_samples_leaf=4,\n                              random_state=46, validation_fraction=0.001,\n                              verbose=999)",
      "text/html": "<style>#sk-container-id-56 {color: black;background-color: white;}#sk-container-id-56 pre{padding: 0;}#sk-container-id-56 div.sk-toggleable {background-color: white;}#sk-container-id-56 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-56 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-56 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-56 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-56 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-56 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-56 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-56 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-56 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-56 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-56 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-56 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-56 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-56 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-56 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-56 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-56 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-56 div.sk-item {position: relative;z-index: 1;}#sk-container-id-56 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-56 div.sk-item::before, #sk-container-id-56 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-56 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-56 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-56 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-56 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-56 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-56 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-56 div.sk-label-container {text-align: center;}#sk-container-id-56 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-56 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-56\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HistGradientBoostingRegressor(l2_regularization=0.02, learning_rate=0.01,\n                              loss=&#x27;absolute_error&#x27;, max_bins=100,\n                              max_iter=10000, min_samples_leaf=4,\n                              random_state=46, validation_fraction=0.001,\n                              verbose=999)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-56\" type=\"checkbox\" checked><label for=\"sk-estimator-id-56\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HistGradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>HistGradientBoostingRegressor(l2_regularization=0.02, learning_rate=0.01,\n                              loss=&#x27;absolute_error&#x27;, max_bins=100,\n                              max_iter=10000, min_samples_leaf=4,\n                              random_state=46, validation_fraction=0.001,\n                              verbose=999)</pre></div></div></div></div></div>"
     },
     "execution_count": 1035,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boost_model = HistGradientBoostingRegressor(learning_rate=0.01, verbose=999, max_depth=None, min_samples_leaf=4,\n",
    "                                            max_iter=10000, loss='absolute_error', l2_regularization=0.02, max_bins=100,\n",
    "                                            validation_fraction=0.001, random_state=46)\n",
    "\n",
    "boost_model.fit(origin_train.drop(['Sales'], axis=1).values, origin_train['Sales'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1036,
   "outputs": [],
   "source": [
    "predicted = postprocess(boost_model.predict(origin_test.values))\n",
    "df_inference = pd.DataFrame(data=predicted, index=range(0, len(predicted)), columns=['Expected'])\n",
    "df_inference.index.name = 'Id'\n",
    "\n",
    "df_inference.to_csv('../submissions/inference.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1037,
   "outputs": [],
   "source": [
    "origin_test.to_csv('../processed-datasets/processed_test-19.58.csv')\n",
    "origin_train.to_csv('../processed-datasets/processed_train-19.58.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
